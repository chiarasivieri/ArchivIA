{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrI-bwVzshRa"
      },
      "outputs": [],
      "source": [
        "!pip install piexif -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpXQFHrKz9ck",
        "outputId": "4e3688ad-d3e7-4acc-b16a-14fc3f9bde8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import piexif\n",
        "import shutil\n",
        "from types import SimpleNamespace"
      ],
      "metadata": {
        "id": "b1oKcFoks83R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "7BO8FPOGtHoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1e4c13-82c6-4524-bec7-7f1de5dafe34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurazione Percorso Base del Progetto - da Drive\n",
        "GDRIVE_PROJECT_PATH = Path(\"/content/drive/MyDrive/PROGETTO\")\n",
        "print(f\"Percorso base del progetto impostato a: {GDRIVE_PROJECT_PATH}\")\n",
        "\n",
        "\n",
        "# CONFIGURAZIONE PARAMETRI GLOBALI\n",
        "# (Questi non cambiano)\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_HEAD = 10\n",
        "EPOCHS_FINE = 8\n",
        "FRAME_INTERVAL = 1\n",
        "LEARN_HEAD_LR = 1e-3\n",
        "LEARN_FULL_LR = 1e-5\n",
        "SHARPNESS_THRESHOLD = 80.0\n",
        "FRAMES_PER_PLACE_PER_VIDEO = 5\n",
        "\n",
        "\n",
        "#PERCORSI DI DEFAULT DERIVATI (DA MODIFICARE)\n",
        "\n",
        "# Percorsi per Training, Validazione e Test\n",
        "DEFAULT_TRAIN_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"train\"\n",
        "DEFAULT_VAL_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"val\"\n",
        "DEFAULT_TEST_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"test\"\n",
        "\n",
        "# Percorsi per l'Inferenza (puntano alla cartella 'test')\n",
        "# In questo modo, quando si esegue l'inferenza, userà le immagini e i video di test.\n",
        "DEFAULT_IMG_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"test\"\n",
        "DEFAULT_VID_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"test\"\n",
        "\n",
        "# Cartella principale dove verranno salvati tutti gli output\n",
        "DEFAULT_OUTPUT = GDRIVE_PROJECT_PATH / \"output\"\n",
        "\n",
        "# Percorsi derivati per l'output\n",
        "DEFAULT_CSV = DEFAULT_OUTPUT / \"results.csv\"\n",
        "PROCESSED_LOG = DEFAULT_OUTPUT / \"processed.log\"\n",
        "BEST_FRAMES_OUTPUT_DIR = DEFAULT_OUTPUT / \"best_frames\"\n",
        "TEMP_FRAME_EXTRACT_DIR = DEFAULT_OUTPUT / \"temp_frames\"\n",
        "\n",
        "\n",
        "#Creazione delle Directory di Output\n",
        "DEFAULT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "BEST_FRAMES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEMP_FRAME_EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# Stampa di Verifica\n",
        "# Aggiornata per includere il nuovo percorso di test\n",
        "print(f\"\\n Percorsi Configurati \")\n",
        "print(f\"Training Data: {DEFAULT_TRAIN_DIR}\")\n",
        "print(f\"Validation Data: {DEFAULT_VAL_DIR}\")\n",
        "print(f\"Test Data: {DEFAULT_TEST_DIR}\")\n",
        "print(f\"Input Immagini/Video (Inferenza): {DEFAULT_IMG_DIR}\")\n",
        "print(f\"Output Generale: {DEFAULT_OUTPUT}\")"
      ],
      "metadata": {
        "id": "sIV5pb3HtJCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a57571e-d8b0-4f5b-ebcd-4407eda8ebc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percorso base del progetto impostato a: /content/drive/MyDrive/PROGETTO\n",
            "\n",
            "--- Percorsi Configurati ---\n",
            "Training Data: /content/drive/MyDrive/PROGETTO/dataset/train\n",
            "Validation Data: /content/drive/MyDrive/PROGETTO/dataset/val\n",
            "Test Data: /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "Input Immagini/Video (Inferenza): /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "Output Generale: /content/drive/MyDrive/PROGETTO/output\n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questa funzione imposta il sistema di logging per il programma.\n",
        "# ( ** Il logging è superiore a `print` perché permette di avere messaggi formattati,\n",
        "# con livelli di gravità diversi (INFO, WARNING, ERROR) e di scrivere\n",
        "# contemporaneamente sia sulla console che su un file di log. )\n",
        "# `output_dir: Path` è un'annotazione di tipo (type hint), indica che la funzione si aspetta un oggetto Path.\n",
        "def setup_logging(output_dir: Path):\n",
        "    # Definisce il percorso del file di log all'interno della cartella di output.\n",
        "    log_file = output_dir / \"run.log\"\n",
        "\n",
        "    # Questo ciclo è FONDAMENTALE in ambienti come Colab/Jupyter.\n",
        "    # Se la cella viene eseguita più volte, `logging.basicConfig` non ha effetto dopo la prima volta.\n",
        "    # Questo codice \"resetta\" il sistema di logging rimuovendo tutti i gestori (handler) esistenti,\n",
        "    # garantendo che la configurazione venga riapplicata correttamente ogni volta.\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "\n",
        "    # Configura il logging a livello base.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,  # Imposta il livello minimo di messaggi da registrare (INFO e superiori).\n",
        "        format=\"%(asctime)s %(levelname)s: %(message)s\",  # Definisce il formato dei messaggi di log.\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',  # Definisce il formato del timestamp.\n",
        "        # Definisce dove inviare i log:\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file, encoding='utf-8'),  # 1: a un file di testo (run.log).\n",
        "            logging.StreamHandler()  # 2: alla console/output della cella.\n",
        "        ]\n",
        "    )\n",
        "    # Registra un primo messaggio per confermare che il logging è attivo e indicare dove si trova il file.\n",
        "    logging.info(f\"Logging configurato. Log salvati in: {log_file}\")\n",
        "\n",
        "\n",
        "# Controlla se una chiave (che rappresenta un file) è già stata processata in precedenza.\n",
        "# Questo evita di ri-elaborare file inutilmente, risparmiando tempo.\n",
        "# `key: str` indica che si aspetta una stringa. `-> bool` indica che restituisce un booleano (True/False).\n",
        "def is_already_processed(key: str) -> bool:\n",
        "    try:\n",
        "        # Se il file di log dei file processati non esiste, nessun file è stato processato.\n",
        "        if not PROCESSED_LOG.exists(): return False\n",
        "        # Apre il file in modalità lettura ('r').\n",
        "        with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:\n",
        "           # Usa un'espressione generatore con `any()` per efficienza.\n",
        "           # `any()` si ferma non appena trova una corrispondenza, senza leggere tutto il file se non necessario.\n",
        "           # `line.strip()` rimuove spazi bianchi e caratteri di a capo dalla riga letta.\n",
        "           return any(key == line.strip() for line in f)\n",
        "    except Exception as e:\n",
        "        # Se si verifica un errore durante la lettura del file, lo registra e assume che il file non sia processato.\n",
        "        logging.error(f\"Errore leggendo {PROCESSED_LOG}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Scrive una chiave nel file di log per marcarla come \"processata\".\n",
        "def mark_processed(key: str):\n",
        "    try:\n",
        "        # Apre il file in modalità 'a' (append), che aggiunge testo alla fine del file senza cancellarlo.\n",
        "        with open(PROCESSED_LOG, \"a\", encoding='utf-8') as f:\n",
        "            # Scrive la chiave seguita da un carattere di a capo.\n",
        "            f.write(key + \"\\n\")\n",
        "    except Exception as e:\n",
        "        # Registra eventuali errori di scrittura.\n",
        "        logging.error(f\"Errore scrivendo su {PROCESSED_LOG}: {e}\")\n",
        "\n",
        "\n",
        "# Estrae frame da un file video a un intervallo di tempo specificato.\n",
        "# `-> list` indica che la funzione restituisce una lista (dei percorsi dei frame salvati).\n",
        "def extract_frames(video_path: Path, frame_output_base_dir: Path, interval_sec: int) -> list:\n",
        "    # Lista per memorizzare i percorsi dei frame salvati.\n",
        "    saved_frames_paths = []\n",
        "    # Crea una sottocartella specifica per i frame di questo video, per tenere tutto ordinato.\n",
        "    # `video_path.stem` è il nome del file senza estensione.\n",
        "    video_frame_dir = frame_output_base_dir / video_path.stem\n",
        "    video_frame_dir.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        # Apre il file video usando OpenCV.\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        # Controlla se il video è stato aperto correttamente.\n",
        "        if not cap.isOpened():\n",
        "            logging.error(f\"Impossibile aprire il video: {video_path}\")\n",
        "            return []  # Restituisce una lista vuota in caso di fallimento.\n",
        "\n",
        "        # Ottiene i fotogrammi al secondo (FPS) del video. Se fallisce, usa 30.0 come default.\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        # Calcola ogni quanti frame leggerne uno, basandosi sull'intervallo in secondi.\n",
        "        step = max(1, int(fps * interval_sec))\n",
        "        # Inizializza i contatori.\n",
        "        frame_count, saved_frame_index, total_frames = 0, 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Usa `tqdm` per creare una barra di avanzamento che mostra il progresso dell'estrazione.\n",
        "        with tqdm(total=total_frames, desc=f\"Extracting frames from {video_path.name}\") as pbar:\n",
        "            while True:\n",
        "                # Legge il frame successivo del video. `ret` è True se la lettura ha successo.\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break  # Esce dal ciclo se il video è terminato.\n",
        "\n",
        "                # Controlla se il contatore di frame è un multiplo dello 'step' calcolato.\n",
        "                if frame_count % step == 0:\n",
        "                    # Costruisce il percorso del file di output per il frame.\n",
        "                    # Il formato `:05d` assicura che il numero del frame abbia sempre 5 cifre (es. 00001, 00012).\n",
        "                    out_file_path = video_frame_dir / f\"{video_path.stem}_frame{saved_frame_index:05d}.jpg\"\n",
        "                    # Salva il frame come immagine JPEG con qualità 90.\n",
        "                    if cv2.imwrite(str(out_file_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 90]):\n",
        "                        saved_frames_paths.append(out_file_path)\n",
        "                        saved_frame_index += 1\n",
        "\n",
        "                frame_count += 1\n",
        "                pbar.update(1)  # Aggiorna la barra di avanzamento.\n",
        "\n",
        "        # Rilascia l'oggetto video per liberare le risorse.\n",
        "        cap.release()\n",
        "        logging.info(f\"Estratti {len(saved_frames_paths)} frame da {video_path.name}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore durante estrazione frame da {video_path}: {e}\")\n",
        "    # Restituisce la lista dei percorsi dei frame salvati.\n",
        "    return saved_frames_paths\n",
        "\n",
        "\n",
        "# Aggiorna i metadati EXIF di un'immagine per includere la label predetta.\n",
        "def update_image_exif(img_path: Path, label: str):\n",
        "    try:\n",
        "        # Apre l'immagine con Pillow.\n",
        "        img = Image.open(img_path)\n",
        "        # Tenta di leggere i dati EXIF esistenti. Se non ce ne sono, usa un byte string vuoto.\n",
        "        exif_data = img.info.get('exif', b\"\")\n",
        "        # Carica i dati EXIF in un dizionario. Se non ci sono dati, crea un dizionario vuoto.\n",
        "        exif_dict = piexif.load(exif_data) if exif_data else {'0th': {}}\n",
        "        # Si assicura che il sotto-dizionario '0th' (che contiene i tag principali) esista.\n",
        "        if '0th' not in exif_dict: exif_dict['0th'] = {}\n",
        "        # Imposta il campo 'ImageDescription' con la nostra etichetta. `piexif` richiede che la stringa sia codificata.\n",
        "        exif_dict['0th'][piexif.ImageIFD.ImageDescription] = label.encode('utf-8')\n",
        "        # Rimuove la thumbnail per evitare problemi di compatibilità e ridurre la dimensione del file.\n",
        "        exif_dict['thumbnail'] = None\n",
        "        # Converte di nuovo il dizionario in formato bytes, pronto per essere scritto.\n",
        "        exif_bytes = piexif.dump(exif_dict)\n",
        "        # Salva l'immagine sovrascrivendo quella vecchia, ma con i nuovi dati EXIF.\n",
        "        img.save(img_path, exif=exif_bytes, quality=95)\n",
        "        # Chiude il file immagine.\n",
        "        img.close()\n",
        "    except Exception as e:\n",
        "        # Se qualcosa va storto (es. file corrotto, formato non supportato), registra un avviso senza bloccare lo script.\n",
        "        logging.warning(f\"UPDATE_EXIF: Errore per {img_path}: {e}\")\n",
        "\n",
        "\n",
        "# Calcola un punteggio di nitidezza per un'immagine.\n",
        "# Utile per filtrare i frame mossi o sfocati.\n",
        "def calculate_sharpness(image_cv2):\n",
        "    try:\n",
        "        # Controllo di sicurezza per evitare errori se l'immagine non è valida.\n",
        "        if image_cv2 is None or image_cv2.size == 0: return 0.0\n",
        "        # Converte l'immagine in scala di grigi, perché la nitidezza è legata ai contorni, non ai colori.\n",
        "        gray = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2GRAY)\n",
        "        # Applica l'operatore Laplaciano, che evidenzia le regioni con rapidi cambi di intensità (i contorni).\n",
        "        # Calcola la varianza del risultato. Un'immagine nitida avrà molti contorni forti e quindi un'alta varianza.\n",
        "        # Un'immagine sfocata avrà bassa varianza.\n",
        "        return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    except Exception as e:\n",
        "        # Gestisce eventuali errori durante le operazioni di OpenCV.\n",
        "        logging.error(f\"CALCULATE_SHARPNESS: Errore: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"Funzioni di Utilità definite.\")"
      ],
      "metadata": {
        "id": "KtwQka6qtOST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f083d947-c1ff-4d28-e699-32838cbb94c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni di Utilità definite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per costruire il modello EfficientNetV2\n",
        "# Questa funzione definisce l'intera architettura della rete neurale che verrà addestrata.\n",
        "# `num_classes: int` indica che si aspetta un intero (il numero di categorie da predire).\n",
        "# `-> models.Model` indica che restituisce un oggetto Modello di Keras.\n",
        "def build_model(num_classes: int) -> models.Model:\n",
        "    \"\"\"Costruisce un modello di classificazione basato su EfficientNetV2B0 pre-addestrato.\"\"\"\n",
        "\n",
        "    # Carica il modello EfficientNetV2B0(CNN)\n",
        "    # Questo modello funge da \"backbone\" (spina dorsale) per l'estrazione delle feature.\n",
        "    backbone = EfficientNetV2B0(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_shape=(*IMG_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # Congela tutti i pesi del backbone\n",
        "    # Impostando `trainable = False`, diciamo a Keras di non aggiornare i pesi del backbone\n",
        "    # durante la prima fase di addestramento. In questo modo, addestriamo solo la \"testa\" che aggiungiamo noi.\n",
        "    backbone.trainable = False\n",
        "\n",
        "    # Costruzione della \"Testa\" di Classificazione\n",
        "    # Questi sono i nuovi layer che mettiamo in cima al backbone.\n",
        "\n",
        "    # 1. Prende l'output del backbone (una mappa di feature multidimensionale).\n",
        "    # 2. `GlobalAveragePooling2D` calcola la media di ogni mappa di feature, trasformandola in un singolo numero.\n",
        "    #    Questo riduce drasticamente il numero di parametri e rende il modello più robusto a piccole traslazioni dell'oggetto.\n",
        "    #    L'output è un vettore di feature per ogni immagine.\n",
        "    x = layers.GlobalAveragePooling2D(name=\"gap\")(backbone.output)\n",
        "\n",
        "    # 3. `Dropout` è un'altra tecnica di regolarizzazione fondamentale. Durante il training, \"spegne\" casualmente\n",
        "    #    il 30% dei neuroni. Questo costringe la rete a non fare troppo affidamento su singoli neuroni\n",
        "    #    e a imparare rappresentazioni più distribuite e robuste.\n",
        "    x = layers.Dropout(0.3, name=\"dropout\")(x)\n",
        "\n",
        "    # 4. `Dense` è un layer \"fully-connected\" standard. Questo è il nostro classificatore finale.\n",
        "    #    - `num_classes`: Il numero di neuroni in output, uno per ogni classe che vogliamo predire.\n",
        "    #    - `activation=\"softmax\"`: La funzione di attivazione Softmax trasforma i punteggi grezzi (logits) del layer\n",
        "    #      in un vettore di probabilità, dove la somma di tutti gli elementi è 1.\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "    # Crea l'oggetto Modello finale, specificando gli input (quelli del backbone) e gli output (quelli del nostro classificatore).\n",
        "    model = models.Model(backbone.input, outputs, name=\"efficientnetv2b0_finetuned\")\n",
        "\n",
        "    # Compilazione del Modello (configura il modello per il training)\n",
        "    model.compile(\n",
        "        # `optimizer`: L'algoritmo che aggiorna i pesi del modello. Adam è una scelta molto comune e robusta.\n",
        "        # Usiamo il learning rate definito per la fase di training della testa.\n",
        "        optimizer=optimizers.Adam(learning_rate=LEARN_HEAD_LR),\n",
        "\n",
        "        # `loss`: La funzione di perdita. Misura quanto le previsioni del modello sono sbagliate.\n",
        "        # `categorical_crossentropy` è la scelta standard per problemi di classificazione multi-classe\n",
        "        # quando le etichette sono in formato one-hot (come fa `label_mode=\"categorical\"`).\n",
        "        loss=\"categorical_crossentropy\",\n",
        "\n",
        "        # `metrics`: Metriche da monitorare durante il training. L'accuracy (precisione) è la più comune.\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Modello EfficientNetV2B0 costruito e compilato per il training della testa.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Pipeline di Data Augmentation\n",
        "# La Data Augmentation crea nuove immagini di training al volo, applicando trasformazioni casuali.\n",
        "# Questo aiuta a prevenire l'overfitting e rende il modello più robusto a variazioni nelle immagini reali.\n",
        "# `tf.keras.Sequential` crea una pipeline dove i dati passano attraverso i layer in sequenza.\n",
        "data_augmentation_pipeline = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),        # Specchia l'immagine orizzontalmente con una probabilità del 50%.\n",
        "    layers.RandomRotation(0.15),            # Ruota l'immagine di un angolo casuale fino al 15% di 360 gradi.\n",
        "    layers.RandomZoom(0.15),                # Ingrandisce o rimpicciolisce l'immagine fino al 15%.\n",
        "    layers.RandomContrast(0.1),             # Modifica il contrasto in modo casuale.\n",
        "    layers.RandomBrightness(0.1),           # Modifica la luminosità in modo casuale.\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "\n",
        "# Funzione per creare i dataset\n",
        "# Questa funzione carica le immagini da una directory, le pre-processa e le prepara in un formato\n",
        "# efficiente per l'addestramento con TensorFlow (`tf.data.Dataset`).\n",
        "def make_dataset(dirpath: Path, shuffle: bool, subset: str = None, validation_split: float = None, augment: bool = False):\n",
        "    # Controllo preliminare per assicurarsi che la directory esista.\n",
        "    if not dirpath.is_dir():\n",
        "        logging.error(f\"MAKE_DATASET: Directory non trovata: {dirpath}\")\n",
        "        return None, None # Restituisce None per segnalare l'errore.\n",
        "    try:\n",
        "        # Funzione di utility di Keras che fa gran parte del lavoro pesante.\n",
        "        # Scansiona la directory, inferisce i nomi delle classi dalle sottocartelle,\n",
        "        # e carica le immagini.\n",
        "        initial_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            dirpath,                      # Il percorso della cartella da cui caricare le immagini.\n",
        "            labels=\"inferred\",            # I nomi delle classi sono dedotti dai nomi delle sottocartelle.\n",
        "            label_mode=\"categorical\",     # Le etichette sono convertite in formato one-hot (es. [0, 1, 0]).\n",
        "            batch_size=BATCH_SIZE,        # Raggruppa le immagini in batch.\n",
        "            image_size=IMG_SIZE,          # Ridimensiona tutte le immagini a IMG_SIZE.\n",
        "            shuffle=shuffle,              # Mescola i dati (importante per il training).\n",
        "            seed=123,                     # Seed per la riproducibilità del mescolamento e dello split.\n",
        "            validation_split=validation_split, # Frazione di dati da riservare per la validazione.\n",
        "            subset=subset,                # Specifica se creare il set di 'training' o 'validation'.\n",
        "            interpolation='bilinear'      # Algoritmo usato per il ridimensionamento delle immagini.\n",
        "        )\n",
        "        # Estrae i nomi delle classi trovati dalla funzione.\n",
        "        class_names = initial_ds.class_names\n",
        "\n",
        "        # Applica la data augmentation solo se richiesto (tipicamente solo per il set di training).\n",
        "        processed_ds = initial_ds\n",
        "        if augment:\n",
        "            # `map` applica una funzione a ogni elemento (batch) del dataset.\n",
        "            # `lambda imgs, labs:` è una funzione anonima che prende un batch di immagini e le loro etichette.\n",
        "            # Applica la pipeline di augmentation solo alle immagini, lasciando le etichette invariate.\n",
        "            # `num_parallel_calls=tf.data.AUTOTUNE` permette a TensorFlow di parallelizzare l'operazione per la massima efficienza.\n",
        "            processed_ds = processed_ds.map(lambda imgs, labs: (data_augmentation_pipeline(imgs, training=True), labs),\n",
        "                                           num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Applica la funzione di preprocessing specifica del modello (es. scalare i pixel da [0, 255] a [-1, 1]).\n",
        "        # Questo passo è OBBLIGATORIO e va fatto sia per il training che per la validazione/inferenza.\n",
        "        processed_ds = processed_ds.map(lambda imgs, labs: (preprocess_input(imgs), labs),\n",
        "                                       num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # `prefetch(tf.data.AUTOTUNE)` è un'ottimizzazione delle performance.\n",
        "        # Permette alla CPU di preparare il batch successivo di dati mentre la GPU sta elaborando quello corrente.\n",
        "        # Questo previene i \"colli di bottiglia\" dovuti al caricamento dei dati.\n",
        "        return processed_ds.prefetch(tf.data.AUTOTUNE), class_names\n",
        "    except Exception as e:\n",
        "        logging.error(f\"MAKE_DATASET: Errore durante creazione dataset da {dirpath}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Messaggio di conferma che le definizioni siano state caricate in memoria.\n",
        "print(\"Funzioni per Modello e Dataset definite.\")"
      ],
      "metadata": {
        "id": "N6wZX4RetTay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d94289a-d1d7-42ac-ecc2-a144b8b59bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni per Modello e Dataset definite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Questa funzione incapsula l'intero processo di addestramento del modello.\n",
        "# Prende un oggetto `args` (che simula gli argomenti da riga di comando)\n",
        "# contenente tutte le configurazioni necessarie.\n",
        "def train(args):\n",
        "    \"\"\"Esegue il ciclo di training completo.\"\"\"\n",
        "    # Inizializza il sistema di logging per questa esecuzione.\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"Inizio Processo di Training\")\n",
        "\n",
        "    # Preparazione dei Dataset\n",
        "    # Converte i percorsi da stringa a oggetti Path per una gestione più semplice.\n",
        "    train_dir_path = Path(args.train_dir)\n",
        "    # Gestisce il caso in cui un percorso di validazione separato non sia fornito.\n",
        "    val_dir_path = Path(args.val_dir) if args.val_dir else None\n",
        "\n",
        "    # Controlla se è stata fornita una directory di validazione separata e se esiste.\n",
        "    if val_dir_path and val_dir_path.is_dir():\n",
        "        # Se sì, crea il dataset di training e quello di validazione da due directory distinte.\n",
        "        logging.info(f\"Uso directory di validazione separata: {val_dir_path}\")\n",
        "        # Per il training: mescola i dati (shuffle=True) e applica data augmentation (augment=True).\n",
        "        train_ds, class_names = make_dataset(train_dir_path, shuffle=True, augment=True)\n",
        "        # Per la validazione: non mescola (per avere risultati consistenti) e non applicare augmentation.\n",
        "        val_ds, _ = make_dataset(val_dir_path, shuffle=False) # (** L'underscore `_` ignora i nomi delle classi, che già abbiamo )\n",
        "    else:\n",
        "        # Se non c'è una cartella di validazione, crea entrambi i set partendo dalla sola cartella di training,\n",
        "        # riservando una frazione dei dati (es. 20%) per la validazione.\n",
        "        logging.info(f\"Uso 20% split da {train_dir_path} per validazione.\")\n",
        "        # `subset='training'` e `validation_split=0.2` dicono a `make_dataset` di creare il set di training con l'80% dei dati.\n",
        "        train_ds, class_names = make_dataset(train_dir_path, shuffle=True, subset='training', validation_split=0.2, augment=True)\n",
        "        # `subset='validation'` crea il set di validazione con il restante 20%.\n",
        "        val_ds, _ = make_dataset(train_dir_path, shuffle=False, subset='validation', validation_split=0.2)\n",
        "\n",
        "    # Controllo di sicurezza: se la creazione di uno dei dataset è fallita, interrompe il training.\n",
        "    if not all([train_ds, val_ds, class_names]):\n",
        "        logging.error(\"Creazione dataset fallita. Training interrotto.\")\n",
        "        return # Esce dalla funzione.\n",
        "\n",
        "    # Recupera il numero di classi e le stampa per verifica.\n",
        "    num_classes = len(class_names)\n",
        "    logging.info(f\"Trovate {num_classes} classi: {class_names}\")\n",
        "\n",
        "    # Costruisce l'architettura del modello chiamando la funzione definita in precedenza.\n",
        "    model = build_model(num_classes)\n",
        "\n",
        "    # Definizione dei Callbacks\n",
        "    # I callbacks sono oggetti che eseguono azioni specifiche in vari momenti del training (es. alla fine di ogni epoca).\n",
        "\n",
        "    # `ModelCheckpoint`: Salva il modello.\n",
        "    checkpoint_head = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(args.output / \"model_head_best.keras\"), # Dove salvare il file.\n",
        "        monitor='val_accuracy',          # Metrica da monitorare per decidere se il modello è \"migliore\".\n",
        "        save_best_only=True,             # Salva solo se la metrica monitorata è migliorata.\n",
        "        mode='max',                      # La 'val_accuracy' deve essere massimizzata.\n",
        "        verbose=1                        # Stampa un messaggio quando il modello viene salvato.\n",
        "    )\n",
        "    # Un secondo checkpoint per la fase di fine-tuning.\n",
        "    checkpoint_fine = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(args.output / \"model_fine_tuned_best.keras\"),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # `EarlyStopping`: Interrompe il training se il modello smette di migliorare.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',              # Monitora la perdita sul set di validazione.\n",
        "        patience=5,                      # Numero di epoche da attendere senza miglioramenti prima di fermarsi.\n",
        "        restore_best_weights=True,       # Al termine, ripristina i pesi del modello alla migliore epoca.\n",
        "        verbose=1                        # Stampa un messaggio quando il training viene interrotto.\n",
        "    )\n",
        "\n",
        "    # FASE 1: Training della sola testa del modello\n",
        "    logging.info(f\"FASE 1: Training testa per {args.epochs_head} epoche\")\n",
        "    # Avvia il training vero e proprio.\n",
        "    model.fit(\n",
        "        train_ds,                        # Dati di training.\n",
        "        validation_data=val_ds,          # Dati di validazione per monitorare le performance.\n",
        "        epochs=args.epochs_head,         # Numero di epoche per questa fase.\n",
        "        callbacks=[checkpoint_head]      # Applica il callback per salvare il miglior modello di questa fase.\n",
        "    )\n",
        "\n",
        "    # Dopo la prima fase, carica esplicitamente il miglior modello salvato dal ModelCheckpoint.\n",
        "    # Questo assicura che si stia partendo dal miglior stato possibile per il fine-tuning,\n",
        "    # anche se l'ultima epoca non era la migliore.\n",
        "    logging.info(\"Caricamento del miglior modello dal training della testa.\")\n",
        "    model = models.load_model(args.output / \"model_head_best.keras\")\n",
        "\n",
        "    # FASE 2: Fine-tuning dell'intero modello\n",
        "    # `model.layers[0]` si riferisce al primo layer del nostro modello, che è il backbone EfficientNetV2.\n",
        "    model.layers[0].trainable = True # Scongela i pesi del backbone, rendendoli aggiornabili.\n",
        "\n",
        "    # Ricompila il modello. Questo è NECESSARIO dopo aver cambiato lo stato `trainable` di un layer.\n",
        "    # Usiamo un learning rate molto più basso per il fine-tuning.\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=args.learn_full_lr),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    logging.info(f\"Modello ricompilato per fine-tuning (LR={args.learn_full_lr})\")\n",
        "\n",
        "    logging.info(f\"FASE 2: Fine-tuning per max {args.epochs_fine} epoche\")\n",
        "    # Avvia la seconda fase di training.\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=args.epochs_fine,\n",
        "        # Usa sia il checkpoint per salvare il miglior modello di questa fase,\n",
        "        # sia l'early stopping per fermarsi se non ci sono più miglioramenti.\n",
        "        callbacks=[checkpoint_fine, early_stopping]\n",
        "    )\n",
        "\n",
        "    # Salvataggio Finale\n",
        "    # Al termine di tutto il processo, salva il modello finale (che, grazie a `restore_best_weights`,\n",
        "    # dovrebbe essere il migliore della fase di fine-tuning) in un percorso standard.\n",
        "    final_model_path = args.output / \"place_model.keras\"\n",
        "    logging.info(f\"Salvataggio modello finale in: {final_model_path}\")\n",
        "    model.save(final_model_path)\n",
        "\n",
        "    # Salva la mappatura tra i nomi delle classi (es. \"Cucina\") e i loro indici numerici (es. 0).\n",
        "    # Questo file è FONDAMENTALE per l'inferenza, per poter tradurre l'output del modello (un indice)\n",
        "    # in un'etichetta leggibile.\n",
        "    class_indices_path = args.output / \"class_indices.csv\"\n",
        "    # Crea un DataFrame pandas e lo salva come CSV.\n",
        "    pd.DataFrame({\"class_name\": class_names, \"index\": list(range(num_classes))}).to_csv(class_indices_path, index=False)\n",
        "    logging.info(f\"Mappatura classi salvata in: {class_indices_path}\")\n",
        "    logging.info(\"Processo di Training Concluso\")\n",
        "\n",
        "# Messaggio di conferma che la funzione `train` è stata definita.\n",
        "print(\"Funzione di Training definita.\")"
      ],
      "metadata": {
        "id": "Tj4VxSfgtWgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d579b32-cb60-4cea-d29b-f7ba112ae55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzione di Training definita.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Blocco di Pulizia Opzionale\n",
        "# Questo blocco di codice, attualmente non in utilizzo,\n",
        "# Serve per resettare lo stato del training, eliminando i modelli e i log delle esecuzioni precedenti.\n",
        "# È utile quando si vuole essere sicuri di iniziare un addestramento completamente da zero.\n",
        "\n",
        "# print(\"Pulizia dei vecchi file di modello e log in corso...\")\n",
        "# # Lista dei nomi dei file di modello che vengono creati durante il training.\n",
        "# old_model_files = [\"model_head_best.keras\", \"model_fine_tuned_best.keras\", \"place_model.keras\"]\n",
        "# # Itera sulla lista per cancellare ogni file.\n",
        "# for f_name in old_model_files:\n",
        "#     # Costruisce il percorso completo del file.\n",
        "#     # `exists()` controlla se il file esiste prima di provare a cancellarlo, per evitare errori.\n",
        "#     if (DEFAULT_OUTPUT / f_name).exists():\n",
        "#         # `.unlink()` è il metodo dell'oggetto Path per cancellare un file.\n",
        "#         (DEFAULT_OUTPUT / f_name).unlink()\n",
        "# # Controlla anche l'esistenza del file di log dei file processati.\n",
        "# if PROCESSED_LOG.exists():\n",
        "#     # E lo cancella se esiste.\n",
        "#     PROCESSED_LOG.unlink()\n",
        "# print(\"Pulizia completata.\")\n",
        "\n",
        "\n",
        "# Configurazione per il Training\n",
        "# Qui creiamo un oggetto che conterrà tutti i parametri da passare alla funzione `train`.\n",
        "# Usiamo `SimpleNamespace` come un modo semplice e veloce per creare un oggetto \"contenitore\"\n",
        "# che si comporta in modo simile all'output di `argparse`, permettendoci di accedere ai valori\n",
        "# con la notazione punto (es. `args.train_dir`).\n",
        "\n",
        "args_train_config = SimpleNamespace(\n",
        "    # Passiamo i percorsi delle directory di training e validazione.\n",
        "    # Vengono convertiti in stringa (`str()`) perché alcune librerie più vecchie\n",
        "    # potrebbero non accettare direttamente gli oggetti Path. È una buona pratica di compatibilità.\n",
        "    train_dir=str(DEFAULT_TRAIN_DIR),\n",
        "    val_dir=str(DEFAULT_VAL_DIR),\n",
        "\n",
        "    # Il percorso della directory di output.\n",
        "    output=DEFAULT_OUTPUT,\n",
        "\n",
        "    # I parametri numerici (iperparametri) definiti nella cella di configurazione globale.\n",
        "    epochs_head=EPOCHS_HEAD,\n",
        "    epochs_fine=EPOCHS_FINE,\n",
        "    learn_head_lr=LEARN_HEAD_LR,\n",
        "    learn_full_lr=LEARN_FULL_LR\n",
        ")\n",
        "\n",
        "\n",
        "# ESECUZIONE DEL TRAINING\n",
        "try:\n",
        "    # Chiama la funzione `train`, passandole l'oggetto di configurazione appena creato\n",
        "    # Da questo momento, il controllo passa alla funzione `train` che eseguirà\n",
        "    # il caricamento dei dati, la costruzione del modello e le due fasi di addestramento\n",
        "    train(args_train_config)\n",
        "\n",
        "# Cattura qualsiasi eccezione (`Exception`) che potrebbe verificarsi durante l'esecuzione\n",
        "# della funzione `train`. Questo previene un crash improvviso del notebook e fornisce\n",
        "# informazioni utili per il debug.\n",
        "except Exception as e:\n",
        "    # Stampa un messaggio di errore chiaro e conciso.\n",
        "    print(f\"ERRORE INASPETTATO DURANTE IL TRAINING: {e}\")\n",
        "    # Importa il modulo `traceback` per ottenere maggiori dettagli sull'errore.\n",
        "    import traceback\n",
        "    # `traceback.print_exc()` stampa la \"traccia dello stack\", che mostra esattamente\n",
        "    # in quale punto del codice e attraverso quale catena di chiamate di funzioni\n",
        "    # si è verificato l'errore. È uno strumento di debug potentissimo.\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "fufQWEKita2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5b0508-fc72-49b0-ac0f-305392d6f5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 09:53:23 INFO: Logging configurato. Log salvati in: /content/drive/MyDrive/PROGETTO/output/run.log\n",
            "2025-07-21 09:53:23 INFO: Inizio Processo di Training\n",
            "2025-07-21 09:53:23 INFO: Uso directory di validazione separata: /content/drive/MyDrive/PROGETTO/dataset/val\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6716 files belonging to 2 classes.\n",
            "Found 321 files belonging to 2 classes.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 09:53:32 INFO: Trovate 2 classi: ['economia_interno', 'stum_interno']\n",
            "2025-07-21 09:53:34 INFO: Modello EfficientNetV2B0 costruito e compilato per il training della testa.\n",
            "2025-07-21 09:53:34 INFO: --- FASE 1: Training testa per 10 epoche ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9235 - loss: 0.1719 \n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/drive/MyDrive/PROGETTO/output/model_head_best.keras\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2426s\u001b[0m 11s/step - accuracy: 0.9238 - loss: 0.1714 - val_accuracy: 1.0000 - val_loss: 0.0157\n",
            "Epoch 2/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0052\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0064\n",
            "Epoch 3/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0022\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
            "Epoch 4/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0014\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
            "Epoch 5/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 8.7516e-04\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 8.7489e-04 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
            "Epoch 6/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 6.5102e-04\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 6.5087e-04 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 7/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.2016e-04\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.2002e-04 - val_accuracy: 1.0000 - val_loss: 9.8271e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.8582e-04\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.8579e-04 - val_accuracy: 1.0000 - val_loss: 8.5546e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.5001e-04\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.4999e-04 - val_accuracy: 1.0000 - val_loss: 7.1094e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.5352e-04\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.5348e-04 - val_accuracy: 1.0000 - val_loss: 5.6637e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-21 11:12:51 INFO: Caricamento del miglior modello dal training della testa.\n",
            "2025-07-21 11:12:52 INFO: Modello ricompilato per fine-tuning (LR=1e-05)\n",
            "2025-07-21 11:12:52 INFO: FASE 2: Fine-tuning per max 8 epoche\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0061\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/drive/MyDrive/PROGETTO/output/model_fine_tuned_best.keras\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0135\n",
            "Epoch 2/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0053\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0121\n",
            "Epoch 3/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0047\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0107\n",
            "Epoch 4/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0043\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0095\n",
            "Epoch 5/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0040\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0086\n",
            "Epoch 6/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0034\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0080\n",
            "Epoch 7/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0031\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0073\n",
            "Epoch 8/8\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0028\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
            "Restoring model weights from the end of the best epoch: 8.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-21 11:49:10 INFO: Salvataggio modello finale in: /content/drive/MyDrive/PROGETTO/output/place_model.keras\n",
            "2025-07-21 11:49:11 INFO: Mappatura classi salvata in: /content/drive/MyDrive/PROGETTO/output/class_indices.csv\n",
            "2025-07-21 11:49:11 INFO: Processo di Training Concluso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# La funzione `infer` è responsabile di usare il modello addestrato per fare previsioni\n",
        "# su nuovi dati (immagini e video).\n",
        "def infer(args):\n",
        "    \"\"\"Esegue inferenza su immagini e video.\"\"\"\n",
        "    # Imposta il logging per questa esecuzione.\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"Inizio Processo di Inferenza\")\n",
        "\n",
        "    # Caricamento del Modello e delle Classi\n",
        "    # Definisce i percorsi del modello salvato e del file CSV con la mappatura delle classi.\n",
        "    model_path = args.output / \"place_model.keras\"\n",
        "    class_indices_path = args.output / \"class_indices.csv\"\n",
        "\n",
        "    # Controlla che entrambi i file necessari esistano. Se mancano, l'inferenza non può procedere.\n",
        "    if not model_path.exists() or not class_indices_path.exists():\n",
        "        logging.error(f\"Modello ({model_path}) o indici classi ({class_indices_path}) non trovati.\")\n",
        "        logging.error(\"Esegui prima il training per creare il modello.\")\n",
        "        return # Interrompe la funzione.\n",
        "\n",
        "    try:\n",
        "        # Carica il modello Keras completo dal file.\n",
        "        model = models.load_model(model_path)\n",
        "        # Legge il file CSV delle classi usando pandas.\n",
        "        class_names_df = pd.read_csv(class_indices_path)\n",
        "        # Assicura che le classi siano ordinate correttamente secondo il loro indice,\n",
        "        # poi le estrae in una lista. Questo garantisce che `class_names[0]` corrisponda\n",
        "        # alla prima classe, `class_names[1]` alla seconda, e così via.\n",
        "        class_names = class_names_df.sort_values(\"index\")[\"class_name\"].tolist()\n",
        "        logging.info(f\"Modello e {len(class_names)} classi caricate: {class_names}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore durante il caricamento del modello o delle classi: {e}\")\n",
        "        return\n",
        "\n",
        "    # Crea la directory di output dove verranno salvate le copie delle immagini classificate.\n",
        "    CLASSIFIED_IMG_OUTPUT_DIR = args.output / \"classified_images\"\n",
        "    CLASSIFIED_IMG_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(f\"Le immagini classificate verranno salvate in: {CLASSIFIED_IMG_OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "    # Preparazione del File CSV di Output\n",
        "    # Apre il file CSV in modalità 'a' (append), per aggiungere nuove righe senza cancellare le vecchie.\n",
        "    # `newline=''` è importante per evitare righe vuote extra nel CSV.\n",
        "    with open(args.csv_path, \"a\", newline=\"\", encoding='utf-8') as csv_file:\n",
        "        # Crea un `DictWriter`, che permette di scrivere righe nel CSV usando dizionari,\n",
        "        # rendendo il codice più leggibile e meno propenso a errori di ordine delle colonne.\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=[\"SourceType\", \"PredictedPlace\", \"Confidence\", \"OriginalPath\", \"OutputPath\", \"IsVideo\", \"Sharpness\"])\n",
        "        # Se il file è vuoto (`csv_file.tell() == 0`), scrive l'intestazione con i nomi delle colonne.\n",
        "        if csv_file.tell() == 0:\n",
        "            writer.writeheader()\n",
        "\n",
        "        # Funzione Helper per Preprocessing Immagini\n",
        "        # Questa funzione interna (nested function) gestisce il caricamento e la preparazione di una singola immagine.\n",
        "        def load_and_preprocess_img_infer(path_str):\n",
        "            try:\n",
        "                # Legge il file immagine dal disco come byte.\n",
        "                img_bytes = tf.io.read_file(path_str)\n",
        "                # Decodifica i byte in un tensore di immagine.\n",
        "                img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
        "                # Ridimensiona l'immagine alla dimensione richiesta dal modello.\n",
        "                img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
        "                # Applica la stessa funzione di preprocessing usata durante il training.\n",
        "                img_preprocessed = preprocess_input(img)\n",
        "                return img_preprocessed\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Errore caricamento immagine {path_str}: {e}\")\n",
        "                return None\n",
        "\n",
        "        # BLOCCO DI INFERENZA SULLE IMMAGINI STATICHE\n",
        "        logging.info(f\"Inizio Inferenza su Immagini Statiche in: {args.img_dir}\")\n",
        "        img_dir = Path(args.img_dir)\n",
        "        if img_dir.is_dir():\n",
        "            # Cerca tutti i file con estensioni di immagine comuni nella directory di input.\n",
        "            image_files = sorted( list(img_dir.glob('*.jpg'))   + list(img_dir.glob('*.jpeg'))  + list(img_dir.glob('*.png'))   + list(img_dir.glob('*.JPG'))   + list(img_dir.glob('*.JPEG'))  + list(img_dir.glob('*.PNG')))\n",
        "            logging.info(f\"Trovate {len(image_files)} immagini da processare.\")\n",
        "\n",
        "            # Itera su ogni immagine trovata, usando `tqdm` per una barra di avanzamento.\n",
        "            for img_path in tqdm(image_files, desc=\"Processing Images\"):\n",
        "                # Crea una chiave univoca per l'immagine per il log dei file processati.\n",
        "                key = f\"IMG::{img_path.resolve()}\"\n",
        "                if is_already_processed(key):\n",
        "                    logging.info(f\"Immagine già processata, saltata: {img_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                # Carica e preprocessa l'immagine.\n",
        "                img_tensor = load_and_preprocess_img_infer(str(img_path))\n",
        "                if img_tensor is None:\n",
        "                    continue # Salta l'immagine se il caricamento fallisce.\n",
        "\n",
        "                # Il modello si aspetta un batch di immagini, non una singola immagine.\n",
        "                # `tf.expand_dims` aggiunge una dimensione all'inizio, trasformando la forma\n",
        "                # da (224, 224, 3) a (1, 224, 224, 3).\n",
        "                img_batch = tf.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "                prediction = model.predict(img_batch, verbose=0)[0]\n",
        "                pred_index = np.argmax(prediction)\n",
        "                confidence = float(np.max(prediction))\n",
        "\n",
        "                if confidence >= 0.50:\n",
        "                    label = class_names[pred_index]\n",
        "                    logging.info(f\"Immagine: {img_path.name} -> Classe: {label} (Conf: {confidence:.3f}) -> ACCETTATA\")\n",
        "\n",
        "                    # Salvataggio e Archiviazione del Risultato\n",
        "                    dest_dir = CLASSIFIED_IMG_OUTPUT_DIR / label\n",
        "                    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    dest_path = dest_dir / img_path.name\n",
        "                    shutil.copy(str(img_path), str(dest_path))\n",
        "                    update_image_exif(dest_path, label)\n",
        "\n",
        "                    # Scrive una riga nel file CSV con tutte le informazioni raccolte.\n",
        "                    writer.writerow({\n",
        "                        \"SourceType\": \"Image\", \"PredictedPlace\": label, \"Confidence\": f\"{confidence:.4f}\",\n",
        "                        \"OriginalPath\": str(img_path.resolve()), \"OutputPath\": str(dest_path.resolve()),\n",
        "                        \"IsVideo\": 0, \"Sharpness\": \"N/A\"\n",
        "                    })\n",
        "                    # Marca l'immagine come processata per non rielaborarla in futuro.\n",
        "                    mark_processed(key)\n",
        "                else:\n",
        "                    # (Opzionale ma raccomandato) Logga le immagini che vengono scartate\n",
        "                    label = class_names[pred_index] # Ottieni comunque la label migliore per il log\n",
        "                    logging.info(f\"Immagine: {img_path.name} -> Classe: {label} (Conf: {confidence:.3f}) -> RIFIUTATA (soglia non raggiunta)\")\n",
        "        # FINE BLOCCO IMMAGINI\n",
        "\n",
        "\n",
        "        # BLOCCO DI INFERENZA SUI VIDEO\n",
        "        logging.info(f\"Inizio Inferenza su Video in: {args.vid_dir} \")\n",
        "        vid_dir = Path(args.vid_dir)\n",
        "        if vid_dir.is_dir():\n",
        "            # Cerca tutti i file video comuni.\n",
        "            video_files = sorted(list(vid_dir.glob(\"*.mp4\")) + list(vid_dir.glob(\"*.mov\")) + list(vid_dir.glob(\"*.avi\")))\n",
        "            logging.info(f\"Trovati {len(video_files)} video da processare.\")\n",
        "\n",
        "            for vid_path in tqdm(video_files, desc=\"Processing Videos\"):\n",
        "                key = f\"VID::{vid_path.resolve()}\"\n",
        "                if is_already_processed(key):\n",
        "                    logging.info(f\"Video già processato, saltato: {vid_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                # Estrae i frame dal video.\n",
        "                temp_video_frame_dir = TEMP_FRAME_EXTRACT_DIR / vid_path.stem\n",
        "                frame_paths = extract_frames(vid_path, temp_video_frame_dir, args.frame_interval)\n",
        "                if not frame_paths: continue\n",
        "\n",
        "                # Preprocessa tutti i frame estratti.\n",
        "                valid_frames_data = [d for d in [load_and_preprocess_img_infer(str(p)) for p in frame_paths] if d is not None]\n",
        "                if not valid_frames_data:\n",
        "                    shutil.rmtree(temp_video_frame_dir, ignore_errors=True) # Pulisce i frame temporanei.\n",
        "                    continue\n",
        "\n",
        "                # Esegue la predizione su tutti i frame in un unico batch per efficienza.\n",
        "                # `tf.stack` converte la lista di tensori in un unico tensore-batch.\n",
        "                predictions = model.predict(tf.stack(valid_frames_data), batch_size=BATCH_SIZE)\n",
        "                pred_indices = np.argmax(predictions, axis=1) # Ottiene l'indice predetto per ogni frame.\n",
        "                pred_confidences = np.max(predictions, axis=1) # Ottiene la confidenza per ogni frame.\n",
        "\n",
        "                # Aggregazione dei Risultati per il Video\n",
        "                # `np.bincount` conta le occorrenze di ogni indice. `argmax` trova l'indice più frequente (voto di maggioranza).\n",
        "                majority_idx = np.bincount(pred_indices).argmax()\n",
        "                video_label = class_names[majority_idx]\n",
        "                # Calcola la confidenza media solo per i frame che hanno votato per la classe di maggioranza.\n",
        "                video_confidence = float(np.mean(pred_confidences[pred_indices == majority_idx]))\n",
        "\n",
        "                logging.info(f\"Video: {vid_path.name} -> Classe: {video_label} (Conf: {video_confidence:.3f})\")\n",
        "\n",
        "                # Selezione e Salvataggio dei Frame Migliori\n",
        "                candidate_frames_info = []\n",
        "                # Rivede tutti i frame...\n",
        "                for i, frame_path in enumerate(frame_paths):\n",
        "                    # ...e considera solo quelli che appartengono alla classe di maggioranza.\n",
        "                    if pred_indices[i] == majority_idx:\n",
        "                        # Calcola la loro nitidezza.\n",
        "                        sharpness = calculate_sharpness(cv2.imread(str(frame_path)))\n",
        "                        # Se è sopra la soglia...\n",
        "                        if sharpness >= args.sharpness_threshold:\n",
        "                            # ...lo aggiunge alla lista dei candidati.\n",
        "                            candidate_frames_info.append((sharpness, frame_path, pred_confidences[i]))\n",
        "\n",
        "                # Ordina i frame candidati in base alla nitidezza, dal più alto al più basso.\n",
        "                candidate_frames_info.sort(key=lambda x: x[0], reverse=True)\n",
        "                # Seleziona i migliori N frame.\n",
        "                best_frames_to_save = candidate_frames_info[:args.frames_per_place_per_video]\n",
        "\n",
        "                saved_best_frame_paths = []\n",
        "                final_sharpness = \"N/A\"\n",
        "                if best_frames_to_save:\n",
        "                    # Crea una cartella di output specifica per i frame di questo video.\n",
        "                    video_best_frames_dir = BEST_FRAMES_OUTPUT_DIR / video_label / vid_path.stem\n",
        "                    video_best_frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    final_sharpness = f\"{best_frames_to_save[0][0]:.1f}\"\n",
        "                    for sharpness, frame_p, conf in best_frames_to_save:\n",
        "                        # Costruisce un nome di file descrittivo.\n",
        "                        out_filename = f\"{vid_path.stem}__{video_label}__frame{frame_p.stem.split('frame')[-1]}_sharp{sharpness:.0f}.jpg\"\n",
        "                        out_path = video_best_frames_dir / out_filename\n",
        "                        shutil.copy(str(frame_p), str(out_path))\n",
        "                        update_image_exif(out_path, video_label)\n",
        "                        saved_best_frame_paths.append(str(out_path.resolve()))\n",
        "\n",
        "                # Scrive i risultati aggregati del video nel file CSV.\n",
        "                writer.writerow({\n",
        "                    \"SourceType\": \"Video\", \"PredictedPlace\": video_label, \"Confidence\": f\"{video_confidence:.4f}\",\n",
        "                    \"OriginalPath\": str(vid_path.resolve()), \"OutputPath\": \";\".join(saved_best_frame_paths) if saved_best_frame_paths else \"N/A\",\n",
        "                    \"IsVideo\": 1, \"Sharpness\": final_sharpness\n",
        "                })\n",
        "                mark_processed(key)\n",
        "                # Pulisce la cartella temporanea dei frame per questo video.\n",
        "                shutil.rmtree(temp_video_frame_dir, ignore_errors=True)\n",
        "        else:\n",
        "            logging.warning(f\"La directory dei video {vid_dir} non esiste.\")\n",
        "        # FINE BLOCCO VIDEO\n",
        "\n",
        "    logging.info(\"Processo di Inferenza Concluso\")"
      ],
      "metadata": {
        "id": "JW_dXv8stkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blocco di Pulizia del Log di Processamento\n",
        "# A differenza del blocco di pulizia nella cella del training, questo è attivo di default.\n",
        "# Il suo scopo è cancellare il file `processed.log` prima di ogni esecuzione.\n",
        "# Questo assicura che l'inferenza venga eseguita su TUTTI i file presenti nelle\n",
        "# cartelle di input, anche se sono già stati analizzati in una sessione precedente.\n",
        "# Se volessi un comportamento \"incrementale\" (analizzare solo i file nuovi),\n",
        "# dovresti commentare o rimuovere questo blocco.\n",
        "\n",
        "# Definisce il percorso del file di log.\n",
        "log_file_path = DEFAULT_OUTPUT / \"processed.log\"\n",
        "# Controlla se il file esiste.\n",
        "if log_file_path.exists():\n",
        "    # Stampa messaggi informativi per l'utente.\n",
        "    print(f\"Cancellazione del vecchio file di log: {log_file_path}\")\n",
        "    # Cancella il file.\n",
        "    log_file_path.unlink()\n",
        "    print(\"Log cancellato. Tutti i file verranno ri-processati.\")\n",
        "\n",
        "\n",
        "# Configurazione per l'Inferenza\n",
        "# Crea un oggetto `SimpleNamespace` per contenere tutti i parametri da passare\n",
        "# alla funzione `infer`. Questo raggruppa ordinatamente la configurazione.\n",
        "args_infer_config = SimpleNamespace(\n",
        "    # Percorsi delle directory di input per immagini e video.\n",
        "    img_dir=str(DEFAULT_IMG_DIR),\n",
        "    vid_dir=str(DEFAULT_VID_DIR),\n",
        "\n",
        "    # Percorsi di output.\n",
        "    output=DEFAULT_OUTPUT,\n",
        "    csv_path=DEFAULT_CSV,\n",
        "\n",
        "    # Parametri specifici per il processamento dei video.\n",
        "    frame_interval=FRAME_INTERVAL,\n",
        "    sharpness_threshold=SHARPNESS_THRESHOLD,\n",
        "    frames_per_place_per_video=FRAMES_PER_PLACE_PER_VIDEO,\n",
        ")\n",
        "\n",
        "# Stampa di Verifica dei Parametri\n",
        "# Stampa un riepilogo delle configurazioni chiave per l'inferenza.\n",
        "# È una buona pratica per avere una conferma visiva che lo script\n",
        "# sta leggendo e scriverà nelle cartelle corrette.\n",
        "print(\"\\n--- Parametri per l'Inferenza ---\")\n",
        "print(f\"Directory Immagini Input: {args_infer_config.img_dir}\")\n",
        "print(f\"Directory Video Input: {args_infer_config.vid_dir}\")\n",
        "print(f\"Directory Output: {args_infer_config.output}\")\n",
        "print(f\"File CSV Risultati: {args_infer_config.csv_path}\")\n",
        "\n",
        "# ESECUZIONE DELL'INFERENZA\n",
        "try:\n",
        "    # Chiama la funzione `infer` con la configurazione appena creata.\n",
        "    # A questo punto, lo script inizierà a caricare il modello e a processare\n",
        "    # i file uno per uno, come definito nella cella 10.\n",
        "    infer(args_infer_config)\n",
        "\n",
        "    # Se la funzione `infer` termina senza errori, stampa un messaggio di successo\n",
        "    # e un riepilogo di dove trovare i risultati. Questo è molto utile per l'utente.\n",
        "    print(f\"\\n--- INFERENZA COMPLETATA ---\")\n",
        "    print(f\"Controlla i risultati nel file CSV: {args_infer_config.csv_path}\")\n",
        "    print(f\"Le immagini classificate si trovano in: {args_infer_config.output / 'classified_images'}\")\n",
        "    print(f\"I frame migliori dei video si trovano in: {BEST_FRAMES_OUTPUT_DIR}\")\n",
        "\n",
        "# Cattura qualsiasi errore imprevisto durante l'inferenza.\n",
        "except Exception as e:\n",
        "    # Stampa un messaggio di errore e la traccia completa dello stack per facilitare il debug.\n",
        "    print(f\"ERRORE INASPETTATO DURANTE L'INFERENZA: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "vozCbw2OtqeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb167ec-a218-423f-9781-834693425975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-21 12:40:18 INFO: Logging configurato. Log salvati in: /content/drive/MyDrive/PROGETTO/output/run.log\n",
            "2025-07-21 12:40:18 INFO: Inizio Processo di Inferenza\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancellazione del vecchio file di log: /content/drive/MyDrive/PROGETTO/output/processed.log\n",
            "Log cancellato. Tutti i file verranno ri-processati.\n",
            "\n",
            "--- Parametri per l'Inferenza ---\n",
            "Directory Immagini Input: /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "Directory Video Input: /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "Directory Output: /content/drive/MyDrive/PROGETTO/output\n",
            "File CSV Risultati: /content/drive/MyDrive/PROGETTO/output/results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-21 12:40:20 INFO: Modello e 2 classi caricate: ['economia_interno', 'stum_interno']\n",
            "2025-07-21 12:40:20 INFO: Le immagini classificate verranno salvate in: /content/drive/MyDrive/PROGETTO/output/classified_images\n",
            "2025-07-21 12:40:20 INFO: Inizio Inferenza su Immagini Statiche in: /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "2025-07-21 12:40:20 INFO: Trovate 21 immagini da processare.\n",
            "Processing Images:   0%|          | 0/21 [00:00<?, ?it/s]2025-07-21 12:40:31 INFO: Immagine: Copia di DSC_0290.JPG -> Classe: economia_interno (Conf: 0.950) -> ACCETTATA\n",
            "Processing Images:   5%|▍         | 1/21 [00:11<03:51, 11.60s/it]2025-07-21 12:40:32 INFO: Immagine: Copia di DSC_0293.JPG -> Classe: economia_interno (Conf: 0.996) -> ACCETTATA\n",
            "Processing Images:  10%|▉         | 2/21 [00:12<01:45,  5.55s/it]2025-07-21 12:40:33 INFO: Immagine: Copia di DSC_4225.JPG -> Classe: economia_interno (Conf: 0.989) -> ACCETTATA\n",
            "Processing Images:  14%|█▍        | 3/21 [00:13<00:57,  3.21s/it]2025-07-21 12:40:34 INFO: Immagine: Copia di DSC_4236.JPG -> Classe: economia_interno (Conf: 0.994) -> ACCETTATA\n",
            "Processing Images:  19%|█▉        | 4/21 [00:13<00:35,  2.10s/it]2025-07-21 12:40:34 INFO: Immagine: Copia di DSC_4249.JPG -> Classe: economia_interno (Conf: 0.998) -> ACCETTATA\n",
            "Processing Images:  24%|██▍       | 5/21 [00:14<00:23,  1.49s/it]2025-07-21 12:40:35 INFO: Immagine: Copia di DSC_4434.JPG -> Classe: economia_interno (Conf: 0.999) -> ACCETTATA\n",
            "Processing Images:  29%|██▊       | 6/21 [00:14<00:18,  1.22s/it]2025-07-21 12:40:35 INFO: Immagine: Copia di DSC_4767.jpg -> Classe: stum_interno (Conf: 0.667) -> ACCETTATA\n",
            "Processing Images:  33%|███▎      | 7/21 [00:15<00:12,  1.10it/s]2025-07-21 12:40:36 INFO: Immagine: Copia di DSC_4768.jpg -> Classe: stum_interno (Conf: 0.514) -> ACCETTATA\n",
            "Processing Images:  38%|███▊      | 8/21 [00:15<00:09,  1.41it/s]2025-07-21 12:40:36 INFO: Immagine: Copia di DSC_4771.jpg -> Classe: stum_interno (Conf: 0.937) -> ACCETTATA\n",
            "Processing Images:  43%|████▎     | 9/21 [00:15<00:06,  1.77it/s]2025-07-21 12:40:36 INFO: Immagine: Copia di DSC_4772.jpg -> Classe: stum_interno (Conf: 0.884) -> ACCETTATA\n",
            "Processing Images:  48%|████▊     | 10/21 [00:15<00:05,  2.15it/s]2025-07-21 12:40:37 INFO: Immagine: Copia di DSC_9765.JPG -> Classe: economia_interno (Conf: 0.971) -> ACCETTATA\n",
            "Processing Images:  52%|█████▏    | 11/21 [00:16<00:05,  1.67it/s]2025-07-21 12:40:38 INFO: Immagine: Copia di DSC_9771.JPG -> Classe: economia_interno (Conf: 0.920) -> ACCETTATA\n",
            "Processing Images:  57%|█████▋    | 12/21 [00:17<00:06,  1.44it/s]2025-07-21 12:40:38 INFO: Immagine: Copia di DSC_9774.JPG -> Classe: economia_interno (Conf: 0.986) -> ACCETTATA\n",
            "Processing Images:  62%|██████▏   | 13/21 [00:18<00:06,  1.33it/s]2025-07-21 12:40:39 INFO: Immagine: Copia di DSC_9776.JPG -> Classe: economia_interno (Conf: 0.992) -> ACCETTATA\n",
            "Processing Images:  67%|██████▋   | 14/21 [00:19<00:05,  1.24it/s]2025-07-21 12:40:40 INFO: Immagine: Copia di DSC_9785.JPG -> Classe: economia_interno (Conf: 0.987) -> ACCETTATA\n",
            "Processing Images:  71%|███████▏  | 15/21 [00:20<00:05,  1.07it/s]2025-07-21 12:40:42 INFO: Immagine: Copia di DSC_9789.JPG -> Classe: economia_interno (Conf: 0.989) -> ACCETTATA\n",
            "Processing Images:  76%|███████▌  | 16/21 [00:22<00:05,  1.03s/it]2025-07-21 12:40:43 INFO: Immagine: Copia di _1002366.JPG -> Classe: economia_interno (Conf: 0.989) -> ACCETTATA\n",
            "Processing Images:  81%|████████  | 17/21 [00:23<00:04,  1.08s/it]2025-07-21 12:40:44 INFO: Immagine: Copia di _1002380.JPG -> Classe: economia_interno (Conf: 0.983) -> ACCETTATA\n",
            "Processing Images:  86%|████████▌ | 18/21 [00:24<00:03,  1.04s/it]2025-07-21 12:40:45 INFO: Immagine: Copia di _1002437.JPG -> Classe: economia_interno (Conf: 0.971) -> ACCETTATA\n",
            "Processing Images:  90%|█████████ | 19/21 [00:25<00:02,  1.02s/it]2025-07-21 12:40:46 INFO: Immagine: Copia di _1002440.JPG -> Classe: economia_interno (Conf: 0.997) -> ACCETTATA\n",
            "Processing Images:  95%|█████████▌| 20/21 [00:26<00:00,  1.01it/s]2025-07-21 12:40:47 INFO: Immagine: Copia di _1002460.JPG -> Classe: economia_interno (Conf: 0.998) -> ACCETTATA\n",
            "Processing Images: 100%|██████████| 21/21 [00:26<00:00,  1.28s/it]\n",
            "2025-07-21 12:40:47 INFO: Inizio Inferenza su Video in: /content/drive/MyDrive/PROGETTO/dataset/test \n",
            "2025-07-21 12:40:47 INFO: Trovati 0 video da processare.\n",
            "Processing Videos: 0it [00:00, ?it/s]\n",
            "2025-07-21 12:40:47 INFO: Processo di Inferenza Concluso\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INFERENZA COMPLETATA ---\n",
            "Controlla i risultati nel file CSV: /content/drive/MyDrive/PROGETTO/output/results.csv\n",
            "Le immagini classificate si trovano in: /content/drive/MyDrive/PROGETTO/output/classified_images\n",
            "I frame migliori dei video si trovano in: /content/drive/MyDrive/PROGETTO/output/best_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NUOVA CELLA - DEFINIZIONE DELLA FUNZIONE DI TEST\n",
        "\n",
        "def test(args):\n",
        "    \"\"\"Esegue la valutazione del modello su un set di dati di test separato.\"\"\"\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"--- Inizio Processo di Test ---\")\n",
        "\n",
        "    model_path = args.output / \"place_model.keras\"\n",
        "    if not model_path.exists():\n",
        "        logging.error(f\"Modello non trovato in: {model_path}. Esegui prima il training.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model = models.load_model(model_path)\n",
        "        logging.info(f\"Modello caricato da: {model_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore caricamento modello: {e}\")\n",
        "        return\n",
        "\n",
        "    test_dir_path = Path(args.test_dir)\n",
        "    if not test_dir_path.is_dir():\n",
        "        logging.error(f\"Directory di test non trovata: {test_dir_path}\")\n",
        "        return\n",
        "\n",
        "    test_ds, class_names = make_dataset(test_dir_path, shuffle=False, augment=False)\n",
        "    if not test_ds:\n",
        "        logging.error(\"Creazione del dataset di test fallita.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Dataset di test caricato con classi: {class_names}\")\n",
        "    logging.info(\"Valutazione del modello sul set di test in corso...\")\n",
        "\n",
        "    results = model.evaluate(test_ds, verbose=1)\n",
        "\n",
        "    logging.info(\"--- Risultati del Test ---\")\n",
        "    print(\"\\n--- Risultati del Test ---\")\n",
        "    print(f\"Loss sul set di test: {results[0]:.4f}\")\n",
        "    print(f\"Accuracy sul set di test: {results[1] * 100:.2f}%\")\n",
        "    logging.info(f\"Loss: {results[0]:.4f} - Accuracy: {results[1] * 100:.2f}%\")\n",
        "    logging.info(\"--- Processo di Test Concluso ---\")\n",
        "\n",
        "print(\"Funzione di Test definita.\")"
      ],
      "metadata": {
        "id": "9jPYFrCTjZ_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086adccd-b12f-4cc9-89de-3743131e15b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Funzione di Test definita.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NUOVA CELLA - ESECUZIONE DEL TEST - DA CONTROLLARE\n",
        "\n",
        "args_test_config = SimpleNamespace(\n",
        "    test_dir=str(DEFAULT_TEST_DIR),\n",
        "    output=DEFAULT_OUTPUT\n",
        ")\n",
        "\n",
        "try:\n",
        "    print(\"\\nLancio della valutazione sul set di test...\")\n",
        "    test(args_test_config)\n",
        "except Exception as e:\n",
        "    print(f\"ERRORE INASPETTATO DURANTE IL TEST: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "4AWJvNctjblE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cba7eb9-85b7-4eaa-84a5-63ef54ab8bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-21 11:49:26 INFO: Logging configurato. Log salvati in: /content/drive/MyDrive/PROGETTO/output/run.log\n",
            "2025-07-21 11:49:26 INFO: --- Inizio Processo di Test ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lancio della valutazione sul set di test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-21 11:49:27 INFO: Modello caricato da: /content/drive/MyDrive/PROGETTO/output/place_model.keras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 files belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-21 11:49:27 ERROR: MAKE_DATASET: Errore durante creazione dataset da /content/drive/MyDrive/PROGETTO/dataset/test: No images found in directory /content/drive/MyDrive/PROGETTO/dataset/test. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')\n",
            "2025-07-21 11:49:27 ERROR: Creazione del dataset di test fallita.\n"
          ]
        }
      ]
    }
  ]
}