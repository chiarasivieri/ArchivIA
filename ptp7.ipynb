{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/chiarasivieri/ArchivIA"
      ],
      "metadata": {
        "id": "5F8W_Ra2fzMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrI-bwVzshRa"
      },
      "outputs": [],
      "source": [
        "!pip install piexif -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpXQFHrKz9ck",
        "outputId": "a7d7973c-40c4-4a61-8495-767bf072c307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import piexif\n",
        "import shutil\n",
        "from types import SimpleNamespace"
      ],
      "metadata": {
        "id": "b1oKcFoks83R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "7BO8FPOGtHoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bb3327-52d4-4bab-a830-b3cdb3facd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configurazione dei Percorsi**\n",
        "\n",
        "* **GDRIVE_PROJECT_PATH**= Path(...): Definisce il percorso principale della\n",
        "cartella del progetto su *Google Drive.* Utilizza la libreria pathlib.\n",
        "* **DEFAULT_TRAIN_DIR** = GDRIVE_PROJECT_PATH / \"dataset\" / \"train\": A partire dal percorso base, costruisce i percorsi specifici per i dati. L'operatore / che funziona correttamente su qualsiasi sistema operativo (Windows, Mac, Linux).\n",
        "* **DEFAULT_TRAIN_DIR:** Cartella con le immagini per l'addestramento.\n",
        "* **DEFAULT_VAL_DIR:** Cartella con le immagini per la validazione (usata durante l'addestramento per monitorare le performance).\n",
        "* **DEFAULT_TEST_DIR:** Cartella con le immagini per il test finale (usata dopo l'addestramento per una valutazione imparziale).\n",
        "* **DEFAULT_IMG_DIR e DEFAULT_VID_DIR:** Cartelle da cui prendere immagini e video per l'inferenza (l'uso pratico del modello). In questo caso, puntano alla cartella test.\n",
        "* **DEFAULT_OUTPUT:** La cartella dove verranno salvati tutti i risultati (modelli, log, CSV, immagini classificate).\n",
        "\n",
        "* **DEFAULT_OUTPUT.mkdir(...):** Questo comando crea fisicamente le cartelle di output su Drive. Argomenti:\n",
        "1. **parents=True:** Se la cartella genitore non esiste (es. PROGETTO), la crea.\n",
        "2. **exist_ok=True:** Se la cartella esiste già, non genera un errore.\n"
      ],
      "metadata": {
        "id": "BuNQEfJFXHOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurazione Percorso Base del Progetto - da Drive\n",
        "GDRIVE_PROJECT_PATH = Path(\"/content/drive/MyDrive/PROGETTO\")\n",
        "print(f\"Percorso base del progetto impostato a: {GDRIVE_PROJECT_PATH}\")\n",
        "\n",
        "\n",
        "# CONFIGURAZIONE PARAMETRI GLOBALI\n",
        "# (Questi non cambiano)\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_HEAD = 10\n",
        "EPOCHS_FINE = 8\n",
        "FRAME_INTERVAL = 1 # L'intervallo in secondi tra i fotogrammi estratti da un video.\n",
        "LEARN_HEAD_LR = 1e-3 #approccio a step discreti. Si inizia con un learning rate (LR) alto e costante per N epoche.\n",
        "# Poi, si stoppa, si cambia manualmente il LR a un valore molto più basso, e si riparte per M epoche\n",
        "LEARN_FULL_LR = 1e-5\n",
        "SHARPNESS_THRESHOLD = 80.0 # La soglia minima di \"nitidezza\" che un frame deve avere per essere considerato valido.\n",
        "FRAMES_PER_PLACE_PER_VIDEO = 5\n",
        "\n",
        "\n",
        "#PERCORSI DI DEFAULT DERIVATI (DA MODIFICARE)\n",
        "\n",
        "# Percorsi per Training, Validazione e Test\n",
        "DEFAULT_TRAIN_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"train\"\n",
        "DEFAULT_VAL_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"val\"\n",
        "DEFAULT_TEST_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"test\"\n",
        "\n",
        "# Percorsi per l'Inferenza (puntano alla cartella 'test')\n",
        "# In questo modo, quando si esegue l'inferenza, userà le immagini e i video di test.\n",
        "DEFAULT_IMG_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"test\"\n",
        "DEFAULT_VID_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"test\"\n",
        "\n",
        "# Cartella principale dove verranno salvati tutti gli output\n",
        "DEFAULT_OUTPUT = GDRIVE_PROJECT_PATH / \"output\"\n",
        "\n",
        "# Percorsi derivati per l'output\n",
        "DEFAULT_CSV = DEFAULT_OUTPUT / \"results.csv\"\n",
        "PROCESSED_LOG = DEFAULT_OUTPUT / \"processed.log\"\n",
        "BEST_FRAMES_OUTPUT_DIR = DEFAULT_OUTPUT / \"best_frames\"\n",
        "TEMP_FRAME_EXTRACT_DIR = DEFAULT_OUTPUT / \"temp_frames\"\n",
        "\n",
        "\n",
        "#Creazione delle Directory di Output\n",
        "DEFAULT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "BEST_FRAMES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEMP_FRAME_EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# Stampa di Verifica\n",
        "# Aggiornata per includere il nuovo percorso di test\n",
        "print(f\"\\n Percorsi Configurati \")\n",
        "print(f\"Training Data: {DEFAULT_TRAIN_DIR}\")\n",
        "print(f\"Validation Data: {DEFAULT_VAL_DIR}\")\n",
        "print(f\"Test Data: {DEFAULT_TEST_DIR}\")\n",
        "print(f\"Input Immagini/Video (Inferenza): {DEFAULT_IMG_DIR}\")\n",
        "print(f\"Output Generale: {DEFAULT_OUTPUT}\")"
      ],
      "metadata": {
        "id": "sIV5pb3HtJCU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce1bae1-1c74-4f81-facc-fd5b73f9213d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percorso base del progetto impostato a: /content/drive/MyDrive/PROGETTO\n",
            "\n",
            " Percorsi Configurati \n",
            "Training Data: /content/drive/MyDrive/PROGETTO/dataset/train\n",
            "Validation Data: /content/drive/MyDrive/PROGETTO/dataset/val\n",
            "Test Data: /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "Input Immagini/Video (Inferenza): /content/drive/MyDrive/PROGETTO/dataset/test\n",
            "Output Generale: /content/drive/MyDrive/PROGETTO/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparazione Dataset + data aug**\n",
        "\n",
        "* `make_dataset`: Trasforma una cartella di immagini in un `tf.data.Dataset`, un oggetto TensorFlow ottimizzato per l'addestramento.\n",
        "\n",
        "*   **`image_dataset_from_directory`**:\n",
        "    * (in teoria) Scansiona la struttura delle directory (es. `/train/economia_interno/img1.jpg`) e inferisce automaticamente che `img1.jpg` appartiene alla classe `economia_interno`.\n",
        "    *   **`label_mode=\"categorical\"`**: È una scelta cruciale. Trasforma le etichette testuali in vettori \"one-hot\" (es. per due classi, `economia_interno` diventa `[1, 0]` e `stum_interno` diventa `[0, 1]`). Questo è il formato matematico richiesto dalla funzione di perdita `categorical_crossentropy` per funzionare correttamente.\n",
        "\n",
        "*   **Catena di Pre-processing con `.map()`**:\n",
        "    * Invece di processare le immagini una per una in un ciclo `for`, il metodo `.map()` applica una funzione a interi batch di dati in modo parallelizzato.\n",
        "    *   **`preprocess_input`**: Ogni modello pre-addestrato, si aspetta che i valori dei pixel delle immagini siano normalizzati in un modo molto specifico (tipo scalati in un range da -1 a 1). La funzione `preprocess_input` fornita da Keras esegue esattamente questa normalizzazione, garantendo che i dati che il nostro modello vede siano nello stesso formato dei dati su cui è stato originariamente addestrato (ImageNet).\n",
        "\n",
        "    *   **`prefetch(tf.data.AUTOTUNE)`**: Permette alla pipeline di dati di lavorare in modo asincrono: mentre la GPU sta addestrando sul batch `N`, la CPU sta già preparando il batch `N+1` in background. `AUTOTUNE` lascia che sia TensorFlow a decidere dinamicamente quanti batch pre-caricare per ottenere le massime prestazioni. (in teoria ottimizza un po' il tutto)"
      ],
      "metadata": {
        "id": "NliIlyTt__Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline di Data Augmentation\n",
        "# La Data Augmentation crea nuove immagini di training al volo, applicando trasformazioni casuali.\n",
        "# Questo aiuta a prevenire l'overfitting e rende il modello più robusto a variazioni nelle immagini reali.\n",
        "# `tf.keras.Sequential` crea una pipeline dove i dati passano attraverso i layer in sequenza.\n",
        "data_augmentation_pipeline = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),        # Specchia l'immagine orizzontalmente con una probabilità del 50%.\n",
        "    layers.RandomRotation(0.15),            # Ruota l'immagine di un angolo casuale fino al 15% di 360 gradi.\n",
        "    layers.RandomZoom(0.15),                # Ingrandisce o rimpicciolisce l'immagine fino al 15%.\n",
        "    layers.RandomContrast(0.1),             # Modifica il contrasto in modo casuale.\n",
        "    layers.RandomBrightness(0.1),           # Modifica la luminosità in modo casuale.\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "\n",
        "# Funzione per creare i dataset\n",
        "# Questa funzione carica le immagini da una directory, le pre-processa e le prepara in un formato\n",
        "# efficiente per l'addestramento con TensorFlow (`tf.data.Dataset`).\n",
        "def make_dataset(dirpath: Path, shuffle: bool, subset: str = None, validation_split: float = None, augment: bool = False):\n",
        "    # Controllo preliminare per assicurarsi che la directory esista.\n",
        "    if not dirpath.is_dir():\n",
        "        logging.error(f\"MAKE_DATASET: Directory non trovata: {dirpath}\")\n",
        "        return None, None # Restituisce None per segnalare l'errore.\n",
        "    try:\n",
        "        # Funzione di utility di Keras che fa gran parte del lavoro pesante.\n",
        "        # Scansiona la directory, inferisce i nomi delle classi dalle sottocartelle,\n",
        "        # e carica le immagini.\n",
        "        initial_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            dirpath,                      # Il percorso della cartella da cui caricare le immagini.\n",
        "            labels=\"inferred\",            # I nomi delle classi sono dedotti dai nomi delle sottocartelle.\n",
        "            label_mode=\"categorical\",     # Le etichette sono convertite in formato one-hot (es. [0, 1, 0]).\n",
        "            batch_size=BATCH_SIZE,        # Raggruppa le immagini in batch.\n",
        "            image_size=IMG_SIZE,          # Ridimensiona tutte le immagini a IMG_SIZE.\n",
        "            shuffle=shuffle,              # Mescola i dati (importante per il training).\n",
        "            seed=123,                     # Seed per la riproducibilità del mescolamento e dello split.\n",
        "            validation_split=validation_split, # Frazione di dati da riservare per la validazione.\n",
        "            subset=subset,                # Specifica se creare il set di 'training' o 'validation'.\n",
        "            interpolation='bilinear'      # Algoritmo usato per il ridimensionamento delle immagini.\n",
        "        )\n",
        "        # Estrae i nomi delle classi trovati dalla funzione.\n",
        "        class_names = initial_ds.class_names\n",
        "\n",
        "        # Applica la data augmentation solo se richiesto (tipicamente solo per il set di training).\n",
        "        processed_ds = initial_ds\n",
        "        if augment:\n",
        "            # `map` applica una funzione a ogni elemento (batch) del dataset.\n",
        "            # `lambda imgs, labs:` è una funzione anonima che prende un batch di immagini e le loro etichette.\n",
        "            # Applica la pipeline di augmentation solo alle immagini, lasciando le etichette invariate.\n",
        "            # `num_parallel_calls=tf.data.AUTOTUNE` permette a TensorFlow di parallelizzare l'operazione per la massima efficienza.\n",
        "            processed_ds = processed_ds.map(lambda imgs, labs: (data_augmentation_pipeline(imgs, training=True), labs),\n",
        "                                           num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Applica la funzione di preprocessing specifica del modello (es. scalare i pixel da [0, 255] a [-1, 1]).\n",
        "        # Questo passo è OBBLIGATORIO e va fatto sia per il training che per la validazione/inferenza.\n",
        "        processed_ds = processed_ds.map(lambda imgs, labs: (preprocess_input(imgs), labs),\n",
        "                                       num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # `prefetch(tf.data.AUTOTUNE)` è un'ottimizzazione delle performance.\n",
        "        # Permette alla CPU di preparare il batch successivo di dati mentre la GPU sta elaborando quello corrente.\n",
        "        # Questo previene i \"colli di bottiglia\" dovuti al caricamento dei dati.\n",
        "        return processed_ds.prefetch(tf.data.AUTOTUNE), class_names\n",
        "    except Exception as e:\n",
        "        logging.error(f\"MAKE_DATASET: Errore durante creazione dataset da {dirpath}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Messaggio di conferma che le definizioni siano state caricate in memoria.\n",
        "print(\"Funzioni per il Dataset definite.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouMX8lIj_SBC",
        "outputId": "b7f2e1aa-fc98-4264-dd13-1deac8688758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni per il Dataset definite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funzioni di Utilità**\n",
        "\n",
        "funzioni per la gestione e l'automazione del flusso di lavoro. L'obiettivo è rendere lo script robusto, efficiente e facile da monitorare.\n",
        "\n",
        "*   **`setup_logging`**: Invece di usare `print`, è stato implementato un sistema di **logging**. Questa scelta è strategica perché:\n",
        "    1.  **Tracciabilità**: Ogni operazione viene registrata con un timestamp e un livello di gravità (INFO, ERROR) in un file di log persistente (`run.log`). Questo permette di analizzare l'esecuzione a posteriori e diagnosticare eventuali problemi, anche per processi che durano ore.\n",
        "    2.  **Doppio Output**: I messaggi vengono mostrati sia in tempo reale sulla console, sia salvati su file, combinando immediatezza e persistenza.\n",
        "\n",
        "*   **`is_already_processed` e `mark_processed`**: Queste due funzioni lavorano insieme per implementare un meccanismo di **caching dello stato di avanzamento**.\n",
        "\n",
        "Durante l'inferenza, lo script potrebbe essere interrotto o rieseguito. Per evitare di ri-analizzare da capo file che hanno già richiesto tempo di calcolo, ogni file processato con successo viene \"marcato\" in un file di registro (`processed.log`).\n",
        "\n",
        "Al successivo avvio, lo script controlla questo registro e salta i file già analizzati."
      ],
      "metadata": {
        "id": "lAsIINc-iNCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **`extract_frames`**: L'analisi di un video da parte di un modello di classificazione di immagini richiede la sua conversione in fotogrammi. Questa funzione automatizza il processo:\n",
        "\n",
        "**Input**:\n",
        "    \n",
        "    1.  `video_path`: Il percorso del file video da processare.\n",
        "\n",
        "    2.  `frame_output_base_dir`: La cartella principale dove verranno salvati i frame.\n",
        "\n",
        "    3.  `interval_sec`: L'intervallo in secondi tra un frame estratto e il successivo (es. `1` per un frame al secondo).\n",
        "*   **Output**:\n",
        "    *   Una **lista** contenente i percorsi completi di tutti i file di immagine (i frame) che sono stati salvati su disco.\n",
        "\n",
        "   *   `fps = cap.get(cv2.CAP_PROP_FPS) or 30.0`: La funzione legge i metadati del video per ottenere il suo framerate (FPS - Fotogrammi al Secondo).\n",
        "    *   `step = max(1, int(fps * interval_sec))`. Calcola ogni quanti frame bisogna salvarne uno.\n",
        "        *   **Es.**: Se un video ha 30 FPS e `interval_sec` è impostato a `2`, lo `step` sarà `60`. Questo significa che la funzione salverà un frame ogni 60 che ne legge, ottenendo di fatto un campionamento di un frame ogni due secondi.\n",
        "\n",
        "4.  **Iterazione e Salvataggio**:\n",
        "    *   Il ciclo `while True` scorre l'intero video, leggendo un frame alla volta con `cap.read()`.\n",
        "    *   `if frame_count % step == 0`: L'operatore % fa da trigger. Solo quando il contatore dei frame è un multiplo esatto dello `step`, il frame corrente viene salvato.\n",
        "    *   `out_file_path = ..._frame{saved_frame_index:05d}.jpg`: Il nome del file di output è formattato in teoria.\n",
        "    *   `cv2.imwrite(...)`: Il frame viene salvato come file JPEG, con un livello di qualità esplicito del 90% per un buon compromesso tra qualità e dimensione del file.\n",
        "\n",
        "5.  **Feedback e Pulizia delle Risorse**:\n",
        "    *   `with tqdm(...) as pbar`: L'intera iterazione è avvolta da `tqdm`, che fornisce la **barra di avanzamento** in tempo reale.\n",
        "    *   `cap.release()`: Al termine, si liberano le risorse\n",
        "\n",
        "\n",
        "    (in realtà ho poi fatto il programma estrazionefile a parte per una questione di comodità)"
      ],
      "metadata": {
        "id": "NcUlP43UiOYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Questa funzione imposta il sistema di logging per il programma.\n",
        "# ( ** Il logging è superiore a `print` perché permette di avere messaggi formattati,\n",
        "# con livelli di gravità diversi (INFO, WARNING, ERROR) e di scrivere\n",
        "# contemporaneamente sia sulla console che su un file di log. )\n",
        "# `output_dir: Path` è un'annotazione di tipo (type hint), indica che la funzione si aspetta un oggetto Path.\n",
        "def setup_logging(output_dir: Path):\n",
        "    # Definisce il percorso del file di log all'interno della cartella di output.\n",
        "    log_file = output_dir / \"run.log\"\n",
        "\n",
        "    # Questo ciclo è FONDAMENTALE in ambienti come Colab/Jupyter.\n",
        "    # Se la cella viene eseguita più volte, `logging.basicConfig` non ha effetto dopo la prima volta.\n",
        "    # Questo codice \"resetta\" il sistema di logging rimuovendo tutti i gestori (handler) esistenti,\n",
        "    # garantendo che la configurazione venga riapplicata correttamente ogni volta.\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "\n",
        "    # Configura il logging a livello base.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,  # Imposta il livello minimo di messaggi da registrare (INFO e superiori).\n",
        "        format=\"%(asctime)s %(levelname)s: %(message)s\",  # Definisce il formato dei messaggi di log.\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',  # Definisce il formato del timestamp.\n",
        "        # Definisce dove inviare i log:\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file, encoding='utf-8'),  # 1: a un file di testo (run.log).\n",
        "            logging.StreamHandler()  # 2: alla console/output della cella.\n",
        "        ]\n",
        "    )\n",
        "    # Registra un primo messaggio per confermare che il logging è attivo e indicare dove si trova il file.\n",
        "    logging.info(f\"Logging configurato. Log salvati in: {log_file}\")\n",
        "\n",
        "\n",
        "# Controlla se una chiave (che rappresenta un file) è già stata processata in precedenza.\n",
        "# Questo evita di ri-elaborare file inutilmente, risparmiando tempo.\n",
        "# `key: str` indica che si aspetta una stringa. `-> bool` indica che restituisce un booleano (True/False).\n",
        "def is_already_processed(key: str) -> bool:\n",
        "    try:\n",
        "        # Se il file di log dei file processati non esiste, nessun file è stato processato.\n",
        "        if not PROCESSED_LOG.exists(): return False\n",
        "        # Apre il file in modalità lettura ('r').\n",
        "        with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:\n",
        "           # Usa un'espressione generatore con `any()` per efficienza.\n",
        "           # `any()` si ferma non appena trova una corrispondenza, senza leggere tutto il file se non necessario.\n",
        "           # `line.strip()` rimuove spazi bianchi e caratteri di a capo dalla riga letta.\n",
        "           return any(key == line.strip() for line in f)\n",
        "    except Exception as e:\n",
        "        # Se si verifica un errore durante la lettura del file, lo registra e assume che il file non sia processato.\n",
        "        logging.error(f\"Errore leggendo {PROCESSED_LOG}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Scrive una chiave nel file di log per marcarla come \"processata\".\n",
        "def mark_processed(key: str):\n",
        "    try:\n",
        "        # Apre il file in modalità 'a' (append), che aggiunge testo alla fine del file senza cancellarlo.\n",
        "        with open(PROCESSED_LOG, \"a\", encoding='utf-8') as f:\n",
        "            # Scrive la chiave seguita da un carattere di a capo.\n",
        "            f.write(key + \"\\n\")\n",
        "    except Exception as e:\n",
        "        # Registra eventuali errori di scrittura.\n",
        "        logging.error(f\"Errore scrivendo su {PROCESSED_LOG}: {e}\")\n",
        "\n",
        "\n",
        "'''\n",
        "# Estrae frame da un file video a un intervallo di tempo specificato.\n",
        "# `-> list` indica che la funzione restituisce una lista (dei percorsi dei frame salvati).\n",
        "def extract_frames(video_path: Path, frame_output_base_dir: Path, interval_sec: int) -> list:\n",
        "    # Lista per memorizzare i percorsi dei frame salvati.\n",
        "    saved_frames_paths = []\n",
        "    # Crea una sottocartella specifica per i frame di questo video, per tenere tutto ordinato.\n",
        "    # `video_path.stem` è il nome del file senza estensione.\n",
        "    video_frame_dir = frame_output_base_dir / video_path.stem\n",
        "    video_frame_dir.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        # Apre il file video usando OpenCV.\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        # Controlla se il video è stato aperto correttamente.\n",
        "        if not cap.isOpened():\n",
        "            logging.error(f\"Impossibile aprire il video: {video_path}\")\n",
        "            return []  # Restituisce una lista vuota in caso di fallimento.\n",
        "\n",
        "        # Ottiene i fotogrammi al secondo (FPS) del video. Se fallisce, usa 30.0 come default.\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        # Calcola ogni quanti frame leggerne uno, basandosi sull'intervallo in secondi.\n",
        "        step = max(1, int(fps * interval_sec))\n",
        "        # Inizializza i contatori.\n",
        "        frame_count, saved_frame_index, total_frames = 0, 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Usa `tqdm` per creare una barra di avanzamento che mostra il progresso dell'estrazione.\n",
        "        with tqdm(total=total_frames, desc=f\"Extracting frames from {video_path.name}\") as pbar:\n",
        "            while True:\n",
        "                # Legge il frame successivo del video. `ret` è True se la lettura ha successo.\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break  # Esce dal ciclo se il video è terminato.\n",
        "\n",
        "                # Controlla se il contatore di frame è un multiplo dello 'step' calcolato.\n",
        "                if frame_count % step == 0:\n",
        "                    # Costruisce il percorso del file di output per il frame.\n",
        "                    # Il formato `:05d` assicura che il numero del frame abbia sempre 5 cifre (es. 00001, 00012).\n",
        "                    out_file_path = video_frame_dir / f\"{video_path.stem}_frame{saved_frame_index:05d}.jpg\"\n",
        "                    # Salva il frame come immagine JPEG con qualità 90.\n",
        "                    if cv2.imwrite(str(out_file_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 90]):\n",
        "                        saved_frames_paths.append(out_file_path)\n",
        "                        saved_frame_index += 1\n",
        "\n",
        "                frame_count += 1\n",
        "                pbar.update(1)  # Aggiorna la barra di avanzamento.\n",
        "\n",
        "        # Rilascia l'oggetto video per liberare le risorse.\n",
        "        cap.release()\n",
        "        logging.info(f\"Estratti {len(saved_frames_paths)} frame da {video_path.name}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore durante estrazione frame da {video_path}: {e}\")\n",
        "    # Restituisce la lista dei percorsi dei frame salvati.\n",
        "    return saved_frames_paths\n",
        "'''\n",
        "    print(\"Funzioni di Utilità iniziali definite.\")"
      ],
      "metadata": {
        "id": "KtwQka6qtOST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`update_image_exif`**\n",
        "\n",
        "Questa funzione è stata progettata per integrare il risultato della classificazione AI direttamente nell'immagine.\n",
        "\n",
        "\n",
        "**1. Lettura Sicura dei Dati EXIF**\n",
        "```python\n",
        "img = Image.open(img_path)\n",
        "exif_data = img.info.get('exif', b\"\")\n",
        "exif_dict = piexif.load(exif_data) if exif_data else {'0th': {}}\n",
        "```\n",
        "**Non tutte le immagini JPEG contengono metadati EXIF.** Un approccio come `img.info['exif']` causerebbe un `KeyError` se i metadati fossero assenti.\n",
        "\n",
        "*   **Implementazione**:\n",
        "    *   `img.info.get('exif', b\"\")`: L'uso del metodo `.get()` per accedere ai dizionari in modo sicuro. Se la chiave `'exif'` non esiste, restituisce un valore predefinito (un oggetto `bytes` vuoto, `b\"\"`) invece di generare un'eccezione. --> modificabile in futuro\n",
        "    *   `if exif_data else ...`: L'operatore ternario controlla se `exif_data` non è vuoto. Solo in quel caso viene chiamato `piexif.load()`. Se è vuoto, viene creata una struttura `exif_dict` minima (`{'0th': {}}`) **per garantire che il codice successivo che tenta di scrivere in `exif_dict['0th']` non fallisca.** Questo rende la funzione **universale**, in grado di aggiungere metadati sia a immagini che ne sono già provviste, sia a immagini che ne sono completamente prive.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Scrittura Strutturata del Tag**\n",
        "```python\n",
        "if '0th' not in exif_dict: exif_dict['0th'] = {}\n",
        "exif_dict['0th'][piexif.ImageIFD.ImageDescription] = label.encode('utf-8')\n",
        "```\n",
        "Lo standard EXIF *è strutturato in sezioni chiamate IFD (Image File Directory).* La sezione principale è la `'0th'`. IMPORTANTE che ci sia\n",
        "*   **Implementazione**:\n",
        "    *   `if '0th' not in exif_dict...`: Questo controllo di sicurezza gestisce il caso in cui un file abbia dati EXIF ma manchi della sezione principale, prevenendo un `KeyError`.\n",
        "\n",
        "    *   `piexif.ImageIFD.ImageDescription`: Invece di usare il codice del tag è 270, penso che sia più chiaro usare una costante nominata(?).\n",
        "    *   `.encode('utf-8')`: Lo standard EXIF richiede che i valori testuali siano memorizzati come `bytes`, non come stringhe Python. L'uso di `.encode('utf-8')` serve garantire la compatibilità, poiché UTF-8 è in grado di rappresentare quasi tutti i caratteri e i simboli internazionali.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Ottimizzazione e Compatibilità**\n",
        "```python\n",
        "exif_dict['thumbnail'] = None\n",
        "exif_bytes = piexif.dump(exif_dict)\n",
        "img.save(img_path, exif=exif_bytes, quality=95)\n",
        "```\n",
        "Ovviamente i dati vanno puliti in modo sicuro\n",
        "*   **Implementazione**:\n",
        "    *   `exif_dict['thumbnail'] = None`: Le thumbnail incorporate nei dati EXIF sono una fonte comune di problemi. Alcuni software le gestiscono male, e possono portare a corruzione dei metadati. Rimuoverle (`None`) è una misura preventiva che aumenta l'affidabilità del file generato e ne riduce leggermente le dimensioni.\n",
        "    *   `piexif.dump()`: Questa funzione esegue l'operazione inversa, riconvertendo il dizionario Python, ora modificato, nel formato binario richiesto dallo standard EXIF.\n",
        "    *   `img.save(..., quality=95)`: Quando si salva un'immagine JPEG, è necessario specificare un livello di qualità. Omettere questo parametro farebbe sì che Pillow utilizzi un valore predefinito che potrebbe degradare la qualità visiva dell'immagine. Impostare esplicitamente `quality=95` è una scelta per preservare la qualità dell'immagine originale il più possibile.\n",
        "\n",
        "  *  un file JPEG non può contenere un dizionario Python. Richiede un formato binario molto specifico, una sequenza di bytes strutturata secondo lo standard EXIF.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Gestione delle Risorse**\n",
        "```python\n",
        "img.close()\n",
        "```\n",
        "Ogni volta che un programma apre un file, occupa una piccola porzione di risorse di sistema (file handle).\n",
        "*   **Implementazione**: `img.close()` chiude esplicitamente il file, assicurando che le risorse vengano liberate. (meglio per programmi che devono lavorare con tanti file rispetto a garbage collection)"
      ],
      "metadata": {
        "id": "3gfBOrwPkW3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggiorna i metadati EXIF di un'immagine per includere la label predetta.\n",
        "def update_image_exif(img_path: Path, label: str):\n",
        "    try:\n",
        "        # Apre l'immagine con Pillow\n",
        "        img = Image.open(img_path)\n",
        "        # Tenta di leggere i dati EXIF esistenti. Se non ce ne sono, usa un byte string vuoto\n",
        "        exif_data = img.info.get('exif', b\"\")\n",
        "        # Carica i dati EXIF in un dizionario. Se non ci sono dati, crea un dizionario vuoto\n",
        "        exif_dict = piexif.load(exif_data) if exif_data else {'0th': {}}\n",
        "        # Si assicura che il sotto-dizionario '0th' (che contiene i tag principali) esista\n",
        "        if '0th' not in exif_dict: exif_dict['0th'] = {}\n",
        "        # Imposta il campo 'ImageDescription' con la nostra etichetta. `piexif` richiede che la stringa sia codificata.\n",
        "        exif_dict['0th'][piexif.ImageIFD.ImageDescription] = label.encode('utf-8')\n",
        "        # Rimuove la thumbnail per evitare problemi di compatibilità e ridurre la dimensione del file.\n",
        "        exif_dict['thumbnail'] = None\n",
        "        # Converte di nuovo il dizionario in formato bytes, pronto per essere scritto.\n",
        "        exif_bytes = piexif.dump(exif_dict)\n",
        "        # Salva l'immagine sovrascrivendo quella vecchia, ma con i nuovi dati EXIF.\n",
        "        img.save(img_path, exif=exif_bytes, quality=95)\n",
        "        # Chiude il file immagine.\n",
        "        img.close()\n",
        "    except Exception as e:\n",
        "        # Se qualcosa va storto (es. file corrotto, formato non supportato), registra un avviso senza bloccare lo script.\n",
        "        logging.warning(f\"UPDATE_EXIF: Errore per {img_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "GZyM4AQJiooq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`calculate_sharpness`**\n",
        "\n",
        "Quando si estraggono frame da un video, specialmente se girato a mano, è inevitabile ottenere immagini di qualità variabile. Alcune saranno nitide, altre saranno mosse o sfocate a causa del movimento della telecamera. Per garantire che il modello di classificazione riceva solo input di alta qualità (principio Garbage In, Garbage Out), è stata implementata la funzione `calculate_sharpness`.\n",
        "\n",
        "In pratica assegna **un punteggio numerico oggettivo alla nitidezza di un'immagine**. Questo punteggio permette di filtrare automaticamente i frame di bassa qualità, scartando quelli che cadono al di sotto di una soglia predefinita (`SHARPNESS_THRESHOLD`).\n",
        "\n",
        "Assicura che i frame selezionati come rappresentativi di una scena siano anche di una qualità buona, migliorando l'affidabilità complessiva del sistema.\n",
        "\n",
        "* **le immagini nitide contengono molti bordi e dettagli ad alta frequenza, mentre le immagini sfocate ne sono prive.** Il metodo quantifica questa idea in tre passaggi:\n",
        "\n",
        "1.  **Conversione in Scala di Grigi (`cv2.cvtColor`)**:\n",
        "    * Trasforma l'immagine a colori in un'immagine in bianco e nero.\n",
        "    * La nitidezza è una proprietà legata ai cambiamenti di luminosità (edges), non ai colori. Lavorare su un singolo canale di colore invece di tre rende il calcolo molto più semplice ed efficiente, senza perdita di informazioni pertinenti per questo specifico compito.\n",
        "\n",
        "2.  **Applicazione dell'Operatore Laplaciano (`cv2.Laplacian`)**:\n",
        "    * Applica un filtro Laplaciano all'immagine in scala di grigi. Questo filtro è un operatore matematico che calcola la derivata seconda dell'immagine. In teoria dovrebbe **rilevare e accentuare le zone in cui i pixel cambiano di intensità molto rapidamente** (bordi e ai dettagli fini).\n",
        "    *  Se potessimo vedere l'immagine dopo il filtro Laplaciano:\n",
        "        *   In un'**immagine nitida**, i bordi degli oggetti apparirebbero come linee bianche e nere molto intense su uno sfondo grigio.\n",
        "        *   In un'**immagine sfocata**, dove i bordi sono sfumati, l'output sarebbe un'immagine per lo più grigia e uniforme, con poche o nessuna linea netta.\n",
        "\n",
        "3.  **Calcolo della Varianza (`.var()`)**:\n",
        "    * Calcola la varianza dei valori dei pixel dell'immagine filtrata dal Laplaciano. La varianza è una misura statistica di quanto i valori in un insieme di dati si discostano dalla media.\n",
        "\n",
        "        *   Nell'**immagine nitida** filtrata, ci sono molti pixel con valori estremi (bianco e nero), molto lontani dal grigio medio. Questo porta a una **varianza alta**.\n",
        "        *   Nell'**immagine sfocata** filtrata, la maggior parte dei pixel è vicina al grigio medio. Questo porta a una **varianza bassa**.\n",
        "        \n",
        "*   `if image_cv2 is None or image_cv2.size == 0`: Previene un errore nel caso in cui venga passata un'immagine non valida o vuota, restituendo un punteggio di `0.0`.\n"
      ],
      "metadata": {
        "id": "_WARrXNFiQ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcola un punteggio di nitidezza per un'immagine.\n",
        "# Utile per filtrare i frame mossi o sfocati.\n",
        "def calculate_sharpness(image_cv2):\n",
        "    try:\n",
        "        # Controllo di sicurezza per evitare errori se l'immagine non è valida.\n",
        "        if image_cv2 is None or image_cv2.size == 0: return 0.0\n",
        "        # Converte l'immagine in scala di grigi, perché la nitidezza è legata ai contorni, non ai colori.\n",
        "        gray = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2GRAY)\n",
        "        # Applica l'operatore Laplaciano, che evidenzia le regioni con rapidi cambi di intensità (i contorni).\n",
        "        # Calcola la varianza del risultato. Un'immagine nitida avrà molti contorni forti e quindi un'alta varianza.\n",
        "        # Un'immagine sfocata avrà bassa varianza.\n",
        "        return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    except Exception as e:\n",
        "        # Gestisce eventuali errori durante le operazioni di OpenCV.\n",
        "        logging.error(f\"CALCULATE_SHARPNESS: Errore: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"Funzioni di nitidezza definita.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHWRo9dviqUY",
        "outputId": "2e06050a-f537-44d9-e8cd-b1226847a2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzioni di nitidezza definita.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`build_model`**\n",
        "\n",
        "#### **1. `EfficientNetV2B0`**\n",
        "\n",
        "```python\n",
        "backbone = EfficientNetV2B0(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    # ...\n",
        ")\n",
        "```\n",
        "*   Dopo diversi tentativi con altre reti neurali preaddestrate --> `EfficientNetV2B0`(ImageNet)\n",
        "*   **`include_top=False`**: Si rimuove la parte finale del modello originale, che è specializzata nel classificare le 1000 classi di ImageNet. In questo modo, conserviamo solo la parte utile del modello: **l'estrattore di feature**.\n",
        "\n",
        "#### **2. Congelamento del Backbone: `backbone.trainable = False`**\n",
        "\n",
        "* Nella prima fase di addestramento, si sfrutta in teoria la conoscenza della backbone senza andare ad alterala. Se si addestra tutto il modello da subito con i nostri dati (relativamente pochi), rischieremmo di rovinare i pesi calibrati di ImageNet.\n",
        "* Congelando il backbone, diciamo a TensorFlow di non modificare i suoi pesi. Questo ci permette di addestrare solo la nuova testa che viene aggiunta dopo.\n",
        "\n",
        "#### **3.**\n",
        "\n",
        "*   **`layers.GlobalAveragePooling2D`**: Questo layer prende la mappa di feature prodotta dal backbone **e la condensa in un singolo vettore**. Serve per ridurre il numero di parametri. (in teoria riduce anche l'overfitting)\n",
        "*   **`layers.Dropout(0.3)`**: Durante l'addestramento, \"spegne\" casualmente il 30% dei neuroni. Questo costringe la rete a non fare eccessivo affidamento su specifici percorsi neurali, ma a imparare rappresentazioni più distribuite e generalizzabili.\n",
        "*   **`layers.Dense(num_classes, activation=\"softmax\")`**: Questo è il classificatore finale. È un layer con un neurone per ogni classe che vogliamo predire. L'attivazione **`softmax`** trasforma i punteggi grezzi del modello in un vettore di probabilità (es. `[0.95, 0.05]`), la cui somma è 1, rendendo l'output facilmente interpretabile.\n",
        "\n",
        "#### **4. Compilazione del Modello**\n",
        "\n",
        "* **`optimizer=optimizers.Adam(...)`** : Adam è una scelta standard per la maggior parte dei problemi.\n",
        "\n",
        "* **`loss=\"categorical_crossentropy\"`** : È la funzione di loss ideale per problemi di classificazione multi-classe."
      ],
      "metadata": {
        "id": "q0CJji4m7Vvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per costruire il modello EfficientNetV2\n",
        "# Questa funzione definisce l'intera architettura della rete neurale che verrà addestrata.\n",
        "# `num_classes: int` indica che si aspetta un intero (il numero di categorie da predire).\n",
        "# `-> models.Model` indica che restituisce un oggetto Modello di Keras.\n",
        "def build_model(num_classes: int) -> models.Model:\n",
        "    \"\"\"Costruisce un modello di classificazione basato su EfficientNetV2B0 pre-addestrato.\"\"\"\n",
        "\n",
        "    # Carica il modello EfficientNetV2B0(CNN)\n",
        "    # Questo modello funge da \"backbone\" (spina dorsale) per l'estrazione delle feature.\n",
        "    backbone = EfficientNetV2B0(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_shape=(*IMG_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # Congela tutti i pesi del backbone\n",
        "    # Impostando `trainable = False`, diciamo a Keras di non aggiornare i pesi del backbone\n",
        "    # durante la prima fase di addestramento. In questo modo, addestriamo solo la \"testa\" che aggiungiamo noi.\n",
        "    backbone.trainable = False\n",
        "\n",
        "    # Costruzione della \"Testa\" di Classificazione\n",
        "    # Questi sono i nuovi layer che mettiamo in cima al backbone.\n",
        "\n",
        "    # 1. Prende l'output del backbone (una mappa di feature multidimensionale).\n",
        "    # 2. `GlobalAveragePooling2D` calcola la media di ogni mappa di feature, trasformandola in un singolo numero.\n",
        "    #    Questo riduce drasticamente il numero di parametri e rende il modello più robusto a piccole traslazioni dell'oggetto.\n",
        "    #    L'output è un vettore di feature per ogni immagine.\n",
        "    x = layers.GlobalAveragePooling2D(name=\"gap\")(backbone.output)\n",
        "\n",
        "    # 3. `Dropout` è un'altra tecnica di regolarizzazione fondamentale. Durante il training, \"spegne\" casualmente\n",
        "    #    il 30% dei neuroni. Questo costringe la rete a non fare troppo affidamento su singoli neuroni\n",
        "    #    e a imparare rappresentazioni più distribuite e robuste.\n",
        "    x = layers.Dropout(0.3, name=\"dropout\")(x)\n",
        "\n",
        "    # 4. `Dense` è un layer \"fully-connected\" standard. Questo è il nostro classificatore finale.\n",
        "    #    - `num_classes`: Il numero di neuroni in output, uno per ogni classe che vogliamo predire.\n",
        "    #    - `activation=\"softmax\"`: La funzione di attivazione Softmax trasforma i punteggi grezzi (logits) del layer\n",
        "    #      in un vettore di probabilità, dove la somma di tutti gli elementi è 1.\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "    # Crea l'oggetto Modello finale, specificando gli input (quelli del backbone) e gli output (quelli del nostro classificatore).\n",
        "    model = models.Model(backbone.input, outputs, name=\"efficientnetv2b0_finetuned\")\n",
        "\n",
        "    # Compilazione del Modello (configura il modello per il training)\n",
        "    model.compile(\n",
        "        # `optimizer`: L'algoritmo che aggiorna i pesi del modello. Adam è una scelta molto comune e robusta.\n",
        "        # Usiamo il learning rate definito per la fase di training della testa.\n",
        "        optimizer=optimizers.Adam(learning_rate=LEARN_HEAD_LR),\n",
        "\n",
        "        # `loss`: La funzione di perdita. Misura quanto le previsioni del modello sono sbagliate.\n",
        "        # `categorical_crossentropy` è la scelta standard per problemi di classificazione multi-classe\n",
        "        # quando le etichette sono in formato one-hot (come fa `label_mode=\"categorical\"`).\n",
        "        loss=\"categorical_crossentropy\",\n",
        "\n",
        "        # `metrics`: Metriche da monitorare durante il training. L'accuracy (precisione) è la più comune.\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Modello EfficientNetV2B0 costruito e compilato per il training della testa.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "N6wZX4RetTay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`train`**\n",
        "Il codice inizia con una gestione dei dataset di training e validazione.\n",
        "* Il modello supporta due scenari:\n",
        "    1.  **Split Separato**: Se viene fornita una cartella di validazione (`val_dir`), questa viene usata interamente per la validazione. Questo è l'approccio migliore perché garantisce che i dati di validazione siano completamente disgiunti da quelli di training fin dall'inizio.\n",
        "    2.  **Split Automatico**: Se la cartella di validazione non viene fornita, lo script riserva automaticamente una porzione (il 20%) dei dati di training per la validazione. Questa è un'opzione comoda per esperimenti rapidi.\n",
        "* La data augmentation (`augment=True`) viene applicata **solo** al set di training. I dati di validazione non devono mai essere aumentati, poiché devono rappresentare i dati reali su cui vogliamo misurare le performance del modello in modo oggettivo ad ogni epoca.\n",
        "\n",
        "\n",
        "*  Si utilizza un learning rate relativamente alto (`LEARN_HEAD_LR`) per accelerare questa fase iniziale di apprendimento.\n",
        "\n",
        "**Fine tuning**\n",
        "* Scongelare il modello intero (`backbone.trainable = True`) e continuare l'addestramento per permettere a tutti i layer di adattarsi al dataset.\n",
        "* Questo permette al backbone di specializzarsi ulteriormente, passando da un estrattore di feature generico a uno ottimizzato per distinguere tra le varie classi.\n",
        "* È **fondamentale** ricompilare il modello e usare un **learning rate molto più basso** (`LEARN_FULL_LR`). Questo previene modifiche drastiche ai pesi pre-addestrati, evitando *il catastrophic forgetting*.\n",
        "\n",
        "#### **3. Automazione e Sicurezza con i Callbacks**\n",
        "\n",
        "Per rendere il processo meno dipendente dalla supervisione manuale, sono stati utilizzati dei `Callbacks` di Keras.\n",
        "\n",
        "*   **`ModelCheckpoint`**:\n",
        "    * Salvano automaticamente il modello alla fine di ogni epoca, ma **solo se** le sue performance sul set di validazione (`monitor='val_accuracy'`) sono migliorate.\n",
        "    *   **Implementazione**: Sono stati creati due checkpoint separati, uno per ogni fase, per salvare il miglior modello della fase di head training(lr alto) e il miglior modello della fase di fine tuning. Questo garantisce che, anche se le ultime epoche dovessero peggiorare, conserveremo sempre la versione più performante del modello.\n",
        "\n",
        "*   **`EarlyStopping`**:\n",
        "    * Monitorare la perdita sul set di validazione (`monitor='val_loss'`) e interrompere automaticamente l'addestramento se questa non migliora per un numero definito di epoche (`patience=5`).\n",
        "    *   **Vantaggi**: Previene l'overfitting e risparmia tempo di calcolo, fermando il processo quando non si stanno più ottenendo benefici.\n",
        "    *   **`restore_best_weights=True`**: Garantisce che, quando il training si ferma, i pesi del modello vengano ripristinati allo stato della migliore epoca, non a quelli dell'ultima.\n",
        "\n",
        "1.  **`place_model.keras`**: Il modello finale, addestrato e pronto per essere utilizzato per l'inferenza.\n",
        "2.  **`class_indices.csv`**: Un file di mappatura che associa i nomi delle classi (es. \"economia_interno\") al loro indice numerico (es. 0). Questo file permette di tradurre l'output numerico del modello (che predice un indice) in un'etichetta leggibile dall'uomo."
      ],
      "metadata": {
        "id": "Fy_hWR-BBnTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Questa funzione incapsula l'intero processo di addestramento del modello.\n",
        "# Prende un oggetto `args` (che simula gli argomenti da riga di comando)\n",
        "# contenente tutte le configurazioni necessarie.\n",
        "def train(args):\n",
        "    \"\"\"Esegue il ciclo di training completo.\"\"\"\n",
        "    # Inizializza il sistema di logging per questa esecuzione.\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"Inizio Processo di Training\")\n",
        "\n",
        "    # Preparazione dei Dataset\n",
        "    # Converte i percorsi da stringa a oggetti Path per una gestione più semplice.\n",
        "    train_dir_path = Path(args.train_dir)\n",
        "    # Gestisce il caso in cui un percorso di validazione separato non sia fornito.\n",
        "    val_dir_path = Path(args.val_dir) if args.val_dir else None\n",
        "\n",
        "    # Controlla se è stata fornita una directory di validazione separata e se esiste.\n",
        "    if val_dir_path and val_dir_path.is_dir():\n",
        "        # Se sì, crea il dataset di training e quello di validazione da due directory distinte.\n",
        "        logging.info(f\"Uso directory di validazione separata: {val_dir_path}\")\n",
        "        # Per il training: mescola i dati (shuffle=True) e applica data augmentation (augment=True).\n",
        "        train_ds, class_names = make_dataset(train_dir_path, shuffle=True, augment=True)\n",
        "        # Per la validazione: non mescola (per avere risultati consistenti) e non applicare augmentation.\n",
        "        val_ds, _ = make_dataset(val_dir_path, shuffle=False) # (** L'underscore `_` ignora i nomi delle classi, che già abbiamo )\n",
        "    else:\n",
        "        # Se non c'è una cartella di validazione, crea entrambi i set partendo dalla sola cartella di training,\n",
        "        # riservando una frazione dei dati (es. 20%) per la validazione.\n",
        "        logging.info(f\"Uso 20% split da {train_dir_path} per validazione.\")\n",
        "        # `subset='training'` e `validation_split=0.2` dicono a `make_dataset` di creare il set di training con l'80% dei dati.\n",
        "        train_ds, class_names = make_dataset(train_dir_path, shuffle=True, subset='training', validation_split=0.2, augment=True)\n",
        "        # `subset='validation'` crea il set di validazione con il restante 20%.\n",
        "        val_ds, _ = make_dataset(train_dir_path, shuffle=False, subset='validation', validation_split=0.2)\n",
        "\n",
        "    # Controllo di sicurezza: se la creazione di uno dei dataset è fallita, interrompe il training.\n",
        "    if not all([train_ds, val_ds, class_names]):\n",
        "        logging.error(\"Creazione dataset fallita. Training interrotto.\")\n",
        "        return # Esce dalla funzione.\n",
        "\n",
        "    # Recupera il numero di classi e le stampa per verifica.\n",
        "    num_classes = len(class_names)\n",
        "    logging.info(f\"Trovate {num_classes} classi: {class_names}\")\n",
        "\n",
        "    # Costruisce l'architettura del modello chiamando la funzione definita in precedenza.\n",
        "    model = build_model(num_classes)\n",
        "\n",
        "    # Definizione dei Callbacks\n",
        "    # I callbacks sono oggetti che eseguono azioni specifiche in vari momenti del training (es. alla fine di ogni epoca).\n",
        "\n",
        "    # `ModelCheckpoint`: Salva il modello.\n",
        "    checkpoint_head = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(args.output / \"model_head_best.keras\"), # Dove salvare il file.\n",
        "        monitor='val_accuracy',          # Metrica da monitorare per decidere se il modello è \"migliore\".\n",
        "        save_best_only=True,             # Salva solo se la metrica monitorata è migliorata.\n",
        "        mode='max',                      # La 'val_accuracy' deve essere massimizzata.\n",
        "        verbose=1                        # Stampa un messaggio quando il modello viene salvato.\n",
        "    )\n",
        "    # Un secondo checkpoint per la fase di fine-tuning.\n",
        "    checkpoint_fine = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(args.output / \"model_fine_tuned_best.keras\"),\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # `EarlyStopping`: Interrompe il training se il modello smette di migliorare.\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',              # Monitora la perdita sul set di validazione.\n",
        "        patience=5,                      # Numero di epoche da attendere senza miglioramenti prima di fermarsi.\n",
        "        restore_best_weights=True,       # Al termine, ripristina i pesi del modello alla migliore epoca.\n",
        "        verbose=1                        # Stampa un messaggio quando il training viene interrotto.\n",
        "    )\n",
        "\n",
        "    # FASE 1: Training della sola testa del modello\n",
        "    logging.info(f\"FASE 1: Training testa per {args.epochs_head} epoche\")\n",
        "    # Avvia il training vero e proprio.\n",
        "    model.fit(\n",
        "        train_ds,                        # Dati di training.\n",
        "        validation_data=val_ds,          # Dati di validazione per monitorare le performance.\n",
        "        epochs=args.epochs_head,         # Numero di epoche per questa fase.\n",
        "        callbacks=[checkpoint_head]      # Applica il callback per salvare il miglior modello di questa fase.\n",
        "    )\n",
        "\n",
        "    # Dopo la prima fase, carica esplicitamente il miglior modello salvato dal ModelCheckpoint.\n",
        "    # Questo assicura che si stia partendo dal miglior stato possibile per il fine-tuning,\n",
        "    # anche se l'ultima epoca non era la migliore.\n",
        "    logging.info(\"Caricamento del miglior modello dal training della testa.\")\n",
        "    model = models.load_model(args.output / \"model_head_best.keras\")\n",
        "\n",
        "    # FASE 2: Fine-tuning dell'intero modello\n",
        "    # `model.layers[0]` si riferisce al primo layer del nostro modello, che è il backbone EfficientNetV2.\n",
        "    model.layers[0].trainable = True # Scongela i pesi del backbone, rendendoli aggiornabili.\n",
        "\n",
        "    # Ricompila il modello. Questo è NECESSARIO dopo aver cambiato lo stato `trainable` di un layer.\n",
        "    # Usiamo un learning rate molto più basso per il fine-tuning.\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=args.learn_full_lr),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    logging.info(f\"Modello ricompilato per fine-tuning (LR={args.learn_full_lr})\")\n",
        "\n",
        "    logging.info(f\"FASE 2: Fine-tuning per max {args.epochs_fine} epoche\")\n",
        "    # Avvia la seconda fase di training.\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=args.epochs_fine,\n",
        "        # Usa sia il checkpoint per salvare il miglior modello di questa fase,\n",
        "        # sia l'early stopping per fermarsi se non ci sono più miglioramenti.\n",
        "        callbacks=[checkpoint_fine, early_stopping]\n",
        "    )\n",
        "\n",
        "    # Salvataggio Finale\n",
        "    # Al termine di tutto il processo, salva il modello finale (che, grazie a `restore_best_weights`,\n",
        "    # dovrebbe essere il migliore della fase di fine-tuning) in un percorso standard.\n",
        "    final_model_path = args.output / \"place_model.keras\"\n",
        "    logging.info(f\"Salvataggio modello finale in: {final_model_path}\")\n",
        "    model.save(final_model_path)\n",
        "\n",
        "    # Salva la mappatura tra i nomi delle classi (es. \"Cucina\") e i loro indici numerici (es. 0).\n",
        "    # Questo file è FONDAMENTALE per l'inferenza, per poter tradurre l'output del modello (un indice)\n",
        "    # in un'etichetta leggibile.\n",
        "    class_indices_path = args.output / \"class_indices.csv\"\n",
        "    # Crea un DataFrame pandas e lo salva come CSV.\n",
        "    pd.DataFrame({\"class_name\": class_names, \"index\": list(range(num_classes))}).to_csv(class_indices_path, index=False)\n",
        "    logging.info(f\"Mappatura classi salvata in: {class_indices_path}\")\n",
        "    logging.info(\"Processo di Training Concluso\")\n",
        "\n",
        "# Messaggio di conferma che la funzione `train` è stata definita.\n",
        "print(\"Funzione di Training definita.\")"
      ],
      "metadata": {
        "id": "Tj4VxSfgtWgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca71815a-8aa0-4585-f437-1881daff43d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funzione di Training definita.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Blocco di Pulizia Opzionale\n",
        "# Questo blocco di codice al momento non utilizzato,\n",
        "# Serve per resettare lo stato del training, eliminando i modelli e i log delle esecuzioni precedenti.\n",
        "# È utile quando si vuole essere sicuri di iniziare un addestramento completamente da zero.\n",
        "\n",
        "# print(\"Pulizia dei vecchi file di modello e log in corso...\")\n",
        "# # Lista dei nomi dei file di modello che vengono creati durante il training.\n",
        "# old_model_files = [\"model_head_best.keras\", \"model_fine_tuned_best.keras\", \"place_model.keras\"]\n",
        "# # Itera sulla lista per cancellare ogni file.\n",
        "# for f_name in old_model_files:\n",
        "#     # Costruisce il percorso completo del file.\n",
        "#     # `exists()` controlla se il file esiste prima di provare a cancellarlo, per evitare errori.\n",
        "#     if (DEFAULT_OUTPUT / f_name).exists():\n",
        "#         # `.unlink()` è il metodo dell'oggetto Path per cancellare un file.\n",
        "#         (DEFAULT_OUTPUT / f_name).unlink()\n",
        "# # Controlla anche l'esistenza del file di log dei file processati.\n",
        "# if PROCESSED_LOG.exists():\n",
        "#     # E lo cancella se esiste.\n",
        "#     PROCESSED_LOG.unlink()\n",
        "# print(\"Pulizia completata.\")\n",
        "\n",
        "\n",
        "# Configurazione per il Training\n",
        "# Qui creiamo un oggetto che conterrà tutti i parametri da passare alla funzione `train`.\n",
        "# Usiamo `SimpleNamespace` come un modo semplice e veloce per creare un oggetto \"contenitore\"\n",
        "# che si comporta in modo simile all'output di `argparse`, permettendoci di accedere ai valori\n",
        "# con la notazione punto (es. `args.train_dir`).\n",
        "\n",
        "args_train_config = SimpleNamespace(\n",
        "    # Passiamo i percorsi delle directory di training e validazione.\n",
        "    # Vengono convertiti in stringa (`str()`) perché alcune librerie più vecchie\n",
        "    # potrebbero non accettare direttamente gli oggetti Path. È una buona pratica di compatibilità.\n",
        "    train_dir=str(DEFAULT_TRAIN_DIR),\n",
        "    val_dir=str(DEFAULT_VAL_DIR),\n",
        "\n",
        "    # Il percorso della directory di output.\n",
        "    output=DEFAULT_OUTPUT,\n",
        "\n",
        "    # I parametri numerici (iperparametri) definiti nella cella di configurazione globale.\n",
        "    epochs_head=EPOCHS_HEAD,\n",
        "    epochs_fine=EPOCHS_FINE,\n",
        "    learn_head_lr=LEARN_HEAD_LR,\n",
        "    learn_full_lr=LEARN_FULL_LR\n",
        ")\n",
        "\n",
        "\n",
        "# ESECUZIONE DEL TRAINING\n",
        "try:\n",
        "    # Chiama la funzione `train`, passandole l'oggetto di configurazione appena creato\n",
        "    # Da questo momento, il controllo passa alla funzione `train` che eseguirà\n",
        "    # il caricamento dei dati, la costruzione del modello e le due fasi di addestramento\n",
        "    train(args_train_config)\n",
        "\n",
        "# Cattura qualsiasi eccezione (`Exception`) che potrebbe verificarsi durante l'esecuzione\n",
        "# della funzione `train`. Questo previene un crash improvviso del notebook e fornisce\n",
        "# informazioni utili per il debug.\n",
        "except Exception as e:\n",
        "    # Stampa un messaggio di errore chiaro e conciso.\n",
        "    print(f\"ERRORE INASPETTATO DURANTE IL TRAINING: {e}\")\n",
        "    # Importa il modulo `traceback` per ottenere maggiori dettagli sull'errore.\n",
        "    import traceback\n",
        "    # `traceback.print_exc()` stampa la \"traccia dello stack\", che mostra esattamente\n",
        "    # in quale punto del codice e attraverso quale catena di chiamate di funzioni\n",
        "    # si è verificato l'errore. È uno strumento di debug potentissimo.\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "fufQWEKita2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b984e0a8-9d4f-4b03-c5bf-258bc13b4010",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-10 01:36:04 INFO: Logging configurato. Log salvati in: /content/drive/MyDrive/PROGETTO/output/run.log\n",
            "2025-09-10 01:36:05 INFO: Inizio Processo di Training\n",
            "2025-09-10 01:36:05 INFO: Uso directory di validazione separata: /content/drive/MyDrive/PROGETTO/dataset/val\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7634 files belonging to 3 classes.\n",
            "Found 338 files belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-10 01:36:23 INFO: Trovate 3 classi: ['economia_esterno', 'economia_interno', 'stum_interno']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-10 01:36:26 INFO: Modello EfficientNetV2B0 costruito e compilato per il training della testa.\n",
            "2025-09-10 01:36:26 INFO: FASE 1: Training testa per 10 epoche\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - accuracy: 0.8937 - loss: 0.2790 \n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/drive/MyDrive/PROGETTO/output/model_head_best.keras\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3483s\u001b[0m 14s/step - accuracy: 0.8941 - loss: 0.2783 - val_accuracy: 1.0000 - val_loss: 0.0198\n",
            "Epoch 2/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0076\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
            "Epoch 3/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0033\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0051\n",
            "Epoch 4/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0021\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
            "Epoch 5/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0013\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
            "Epoch 6/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 9.8437e-04\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 9.8411e-04 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
            "Epoch 7/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.2433e-04\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.2422e-04 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
            "Epoch 8/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.5196e-04\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.5206e-04 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
            "Epoch 9/10\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.0045e-04\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.0039e-04 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 10/10\n",
            "\u001b[1m208/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.5969e-04"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`infer`**\n",
        "\n",
        "`infer` prende il modello addestrato e lo utilizza per classificare nuove immagini e video. È stata progettata per non fare solo la previsione, ma gestire anche l'organizzazione dei risultati e l'arricchimento dei dati di output.\n",
        "\n",
        "#### **1. Setup**\n",
        "\n",
        "Prima di iniziare, la funzione esegue controlli preliminari essenziali per la stabilità del processo:\n",
        "\n",
        "*   **Caricamento degli Artefatti**: Carica i file prodotti dalla fase di training: il modello salvato (`place_model.keras`) e la mappatura delle classi (`class_indices.csv`).\n",
        "*   **Controlli di Esistenza**: Verifica che questi file esistano. Invece di generare un errore, invita l'utente a eseguire prima il training.\n",
        "*   **Ordinamento Classi**: l'ordine delle classi (`class_names`) deve corrispondere esattamente agli indici di output del modello (dove l'output `0` corrisponde alla prima classe, `1` alla seconda, etc.). Il codice assicura questo ordinamento leggendo il CSV e ordinandolo per la colonna `index`.\n",
        "\n",
        "#### **2. Processo di Inferenza su Immagini Statiche**\n",
        "\n",
        "qui si gestisce la classificazione delle singole immagini.\n",
        "\n",
        "* Per ogni immagine, viene chiamata la funzione `load_and_preprocess_img_infer`. Questa funzione esegue gli stessi identici passaggi di pre-processing usati durante l'addestramento (ridimensionamento a `IMG_SIZE` e normalizzazione con `preprocess_input`). **Questa coerenza è molto importante**: il modello deve vedere i nuovi dati nello stesso identico formato dei dati su cui è stato addestrato.\n",
        "*   **Predizione**:\n",
        "    *   `tf.expand_dims`: Il modello si aspetta di ricevere un \"batch\" di immagini, non una singola immagine. **Questa funzione aggiunge una dimensione extra, trasformando un'immagine singola in un batch di dimensione 1.** --> Quando carichi e pre-processi una singola immagine, ottieni un tensore con questa forma a 3 dimensioni: (224, 224, 3). Se provi a dare questo tensore direttamente al modello con model.predict(...), TensorFlow ti da errore.\n",
        "    *   `model.predict()`: Esegue la predizione vera e propria. L'output è un vettore di probabilità (es. `[0.05, 0.95]`).\n",
        "    *   `np.argmax()`: Trova l'indice del valore più alto nel vettore di probabilità, che corrisponde all'indice della classe predetta.\n",
        "    \n",
        "## **Filtraggio per Confidenza**\n",
        "Una previsione viene **accettata solo se la sua confidenza è superiore a una soglia (50% --> che comunque verrà alzata, per adesso funziona)**. Questo dovrebbe servire per ridurre i falsi positivi. Se il modello non è \"sicuro\" della sua previsione, il risultato viene scartato (ma comunque registrato nei log per analisi future).\n",
        "*   **Organizzazione dell'Output**: Le immagini accettate vengono:\n",
        "    1.  Copiate in una sottocartella di output il cui nome è la classe predetta (es. `/output/classified_images/economia_interno/`). (cosa che verrà modificata/eliminata una volta che si lavorerà con il NAS)\n",
        "    2.  Arricchite con i metadati EXIF tramite `update_image_exif`:\n",
        "    `update_image_exif(dest_path, label)`\n",
        "    3.  Registrate con tutti i dettagli in un file CSV (`results.csv`) per una analisi successiva.\n",
        "\n",
        "#### **3. Processo di Inferenza su Video**\n",
        "\n",
        "*   **Da Video a Frame**: Il primo passo è scomporre il video in una sequenza di fotogrammi usando la funzione `extract_frames`. (per i video usati per ricavare le immagini che ho usato per addestrare il modello, ho fatto un programma a parte, estrazioneframe.ipynb, è su github)\n",
        "*   **Predizione in Batch**: Per massimizzare l'efficienza, tutti i frame estratti vengono pre-processati e poi dati in pasto al modello in **un unico batch**. Questo sfrutta la capacità della GPU di eseguire calcoli in parallelo ed è molto più veloce che processare i frame uno per uno.\n",
        "*  **Conteggio delle Occorrenze**: Viene calcolato quante volte ogni classe (es. `economia_interno`, `stum_interno`) è stata predetta.\n",
        "    * **Filtro a Soglia**: Viene definita una soglia di presenza (es. 10%). Ogni classe il cui conteggio supera questa soglia rispetto al numero totale di frame viene aggiunta a una lista di \"luoghi riconosciuti\".\n",
        "    *  **Output Multi-Etichetta**: Il risultato finale per il video non è una singola etichetta, ma un elenco di tutte le classi che hanno superato la soglia. Ad esempio, `economia_interno, stum_interno`.\n",
        "\n",
        "Quindi, Se un video mostra due aule diverse, il sistema le riporterà entrambe, fornendo un riassunto completo del contenuto.\n",
        "    *   **Implementazione**: `np.bincount(pred_indices).argmax()` è un modo in NumPy per eseguire questo conteggio e trovare la classe vincente.\n",
        "\n",
        "Sui frame candidati, viene calcolato il punteggio di nitidezza (`calculate_sharpness`).\n",
        "    *   **Output Finale**: Vengono salvati solo i **top N frame** (definito da `FRAMES_PER_PLACE_PER_VIDEO`), ordinati per nitidezza decrescente. Questo garantisce che l'output non sia solo una rappresentazione corretta del contenuto del video.\n",
        "\n",
        "*   **Tracciamento e Pulizia**: Anche per i video, i risultati aggregati vengono salvati nel file CSV. Infine, la cartella temporanea contenente tutti i frame estratti viene eliminata per liberare spazio."
      ],
      "metadata": {
        "id": "hvrVVxlHGUtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La funzione `infer` è responsabile di usare il modello addestrato per fare previsioni\n",
        "# su nuovi dati (immagini e video).\n",
        "def infer(args):\n",
        "    \"\"\"Esegue inferenza su immagini e video.\"\"\"\n",
        "    # Imposta il logging per questa esecuzione.\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"Inizio Processo di Inferenza\")\n",
        "\n",
        "    # Caricamento del Modello e delle Classi\n",
        "    model_path = args.output / \"place_model.keras\"\n",
        "    class_indices_path = args.output / \"class_indices.csv\"\n",
        "\n",
        "    if not model_path.exists() or not class_indices_path.exists():\n",
        "        logging.error(f\"Modello ({model_path}) o indici classi ({class_indices_path}) non trovati.\")\n",
        "        logging.error(\"Esegui prima il training per creare il modello.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model = models.load_model(model_path)\n",
        "        class_names_df = pd.read_csv(class_indices_path)\n",
        "        class_names = class_names_df.sort_values(\"index\")[\"class_name\"].tolist()\n",
        "        logging.info(f\"Modello e {len(class_names)} classi caricate: {class_names}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore durante il caricamento del modello o delle classi: {e}\")\n",
        "        return\n",
        "\n",
        "    CLASSIFIED_IMG_OUTPUT_DIR = args.output / \"classified_images\"\n",
        "    CLASSIFIED_IMG_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    logging.info(f\"Le immagini classificate verranno salvate in: {CLASSIFIED_IMG_OUTPUT_DIR}\")\n",
        "\n",
        "    with open(args.csv_path, \"a\", newline=\"\", encoding='utf-8') as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=[\"SourceType\", \"PredictedPlace\", \"Confidence\", \"OriginalPath\", \"OutputPath\", \"IsVideo\", \"Sharpness\"])\n",
        "        if csv_file.tell() == 0:\n",
        "            writer.writeheader()\n",
        "\n",
        "        def load_and_preprocess_img_infer(path_str):\n",
        "            try:\n",
        "                img_bytes = tf.io.read_file(path_str)\n",
        "                img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
        "                img = tf.image.resize(img, IMG_SIZE, method='bilinear')\n",
        "                img_preprocessed = preprocess_input(img)\n",
        "                return img_preprocessed\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Errore caricamento immagine {path_str}: {e}\")\n",
        "                return None\n",
        "\n",
        "        # BLOCCO DI INFERENZA SULLE IMMAGINI STATICHE\n",
        "        logging.info(f\"Inizio Inferenza su Immagini Statiche in: {args.img_dir}\")\n",
        "        img_dir = Path(args.img_dir)\n",
        "        if img_dir.is_dir():\n",
        "            image_files = sorted( list(img_dir.glob('*.jpg'))   + list(img_dir.glob('*.jpeg'))  + list(img_dir.glob('*.png'))   + list(img_dir.glob('*.JPG'))   + list(img_dir.glob('*.JPEG'))  + list(img_dir.glob('*.PNG')))\n",
        "            logging.info(f\"Trovate {len(image_files)} immagini da processare.\")\n",
        "\n",
        "            for img_path in tqdm(image_files, desc=\"Processing Images\"):\n",
        "                key = f\"IMG::{img_path.resolve()}\"\n",
        "                if is_already_processed(key):\n",
        "                    logging.info(f\"Immagine già processata, saltata: {img_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                img_tensor = load_and_preprocess_img_infer(str(img_path))\n",
        "                if img_tensor is None:\n",
        "                    continue\n",
        "\n",
        "\n",
        "            # Il modello si aspetta un batch di immagini, non una singola immagine.\n",
        "            # `tf.expand_dims` aggiunge una dimensione all'inizio, trasformando la forma\n",
        "            # da (224, 224, 3) a (1, 224, 224, 3).\n",
        "                img_batch = tf.expand_dims(img_tensor, axis=0)\n",
        "                prediction = model.predict(img_batch, verbose=0)[0]\n",
        "                pred_index = np.argmax(prediction)\n",
        "                confidence = float(np.max(prediction))\n",
        "\n",
        "                if confidence >= 0.50:\n",
        "                    label = class_names[pred_index]\n",
        "                    logging.info(f\"Immagine: {img_path.name} -> Classe: {label} (Conf: {confidence:.3f}) -> ACCETTATA\")\n",
        "                    dest_dir = CLASSIFIED_IMG_OUTPUT_DIR / label\n",
        "                    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    dest_path = dest_dir / img_path.name\n",
        "                    shutil.copy(str(img_path), str(dest_path))\n",
        "                    update_image_exif(dest_path, label)\n",
        "\n",
        "                    writer.writerow({\n",
        "                        \"SourceType\": \"Image\", \"PredictedPlace\": label, \"Confidence\": f\"{confidence:.4f}\",\n",
        "                        \"OriginalPath\": str(img_path.resolve()), \"OutputPath\": str(dest_path.resolve()),\n",
        "                        \"IsVideo\": 0, \"Sharpness\": \"N/A\"\n",
        "                    })\n",
        "                    mark_processed(key)\n",
        "                else:\n",
        "                    label = class_names[pred_index]\n",
        "                    logging.info(f\"Immagine: {img_path.name} -> Classe: {label} (Conf: {confidence:.3f}) -> RIFIUTATA (soglia non raggiunta)\")\n",
        "\n",
        "        # BLOCCO DI INFERENZA SUI VIDEO\n",
        "        logging.info(f\"Inizio Inferenza su Video in: {args.vid_dir} \")\n",
        "        vid_dir = Path(args.vid_dir)\n",
        "        if vid_dir.is_dir():\n",
        "            video_files = sorted(list(vid_dir.glob(\"*.mp4\")) + list(vid_dir.glob(\"*.mov\")) + list(vid_dir.glob(\"*.avi\")))\n",
        "            logging.info(f\"Trovati {len(video_files)} video da processare.\")\n",
        "\n",
        "            for vid_path in tqdm(video_files, desc=\"Processing Videos\"):\n",
        "                key = f\"VID::{vid_path.resolve()}\"\n",
        "                if is_already_processed(key):\n",
        "                    logging.info(f\"Video già processato, saltato: {vid_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                temp_video_frame_dir = TEMP_FRAME_EXTRACT_DIR / vid_path.stem\n",
        "                frame_paths = extract_frames(vid_path, temp_video_frame_dir, args.frame_interval)\n",
        "                if not frame_paths: continue\n",
        "\n",
        "                valid_frames_data = [d for d in [load_and_preprocess_img_infer(str(p)) for p in frame_paths] if d is not None]\n",
        "                if not valid_frames_data:\n",
        "                    shutil.rmtree(temp_video_frame_dir, ignore_errors=True)\n",
        "                    continue\n",
        "\n",
        "                predictions = model.predict(tf.stack(valid_frames_data), batch_size=BATCH_SIZE)\n",
        "                pred_indices = np.argmax(predictions, axis=1)\n",
        "                pred_confidences = np.max(predictions, axis=1)\n",
        "\n",
        "                # 1. Conta le occorrenze per ogni classe\n",
        "                counts = np.bincount(pred_indices, minlength=len(class_names))\n",
        "                total_valid_frames = len(pred_indices)\n",
        "\n",
        "                # 2. Definisci una soglia di presenza (es. 10% dei frame) per considerare una scena significativa\n",
        "                presence_threshold_percentage = 0.10\n",
        "\n",
        "                # 3. Identifica gli indici di tutte le classi che superano la soglia\n",
        "                significant_class_indices = [\n",
        "                    idx for idx, count in enumerate(counts)\n",
        "                    if (count / total_valid_frames) >= presence_threshold_percentage\n",
        "                ]\n",
        "\n",
        "                # Se nessuna classe supera la soglia, come fallback si può prendere quella più votata\n",
        "                if not significant_class_indices:\n",
        "                    logging.warning(f\"Nessuna classe ha superato la soglia di presenza per {vid_path.name}. Uso la classe più votata come fallback.\")\n",
        "                    significant_class_indices = [np.argmax(counts)]\n",
        "\n",
        "                # 4. Inizializza le liste per raccogliere i risultati aggregati del video\n",
        "                final_video_labels = []\n",
        "                all_best_frames_paths = []\n",
        "                confidence_scores = []\n",
        "                sharpness_scores = []\n",
        "\n",
        "                # 5. Itera su OGNI classe significativa trovata e seleziona i frame migliori per ciascuna\n",
        "                for class_idx in significant_class_indices:\n",
        "                    video_label = class_names[class_idx]\n",
        "                    final_video_labels.append(video_label)\n",
        "\n",
        "                    # Calcola la confidenza media SOLO per i frame appartenenti a QUESTA classe\n",
        "                    class_confidence = float(np.mean(pred_confidences[pred_indices == class_idx]))\n",
        "                    confidence_scores.append(f\"{video_label}:{class_confidence:.3f}\")\n",
        "                    logging.info(f\"Video: {vid_path.name} -> Rilevata classe significativa: {video_label} (Presenza: {counts[class_idx]/total_valid_frames:.1%}, Conf. media: {class_confidence:.3f})\")\n",
        "\n",
        "                    # Seleziona i frame migliori PER QUESTA CLASSE\n",
        "                    candidate_frames_info = []\n",
        "                    for i, frame_path in enumerate(frame_paths):\n",
        "                        if pred_indices[i] == class_idx:\n",
        "                            sharpness = calculate_sharpness(cv2.imread(str(frame_path)))\n",
        "                            if sharpness >= args.sharpness_threshold:\n",
        "                                candidate_frames_info.append((sharpness, frame_path))\n",
        "\n",
        "                    candidate_frames_info.sort(key=lambda x: x[0], reverse=True)\n",
        "                    best_frames_to_save = candidate_frames_info[:args.frames_per_place_per_video]\n",
        "\n",
        "                    if best_frames_to_save:\n",
        "                        video_best_frames_dir = BEST_FRAMES_OUTPUT_DIR / video_label / vid_path.stem\n",
        "                        video_best_frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "                        sharpness_scores.append(f\"{video_label}:{best_frames_to_save[0][0]:.1f}\")\n",
        "\n",
        "                        for sharpness, frame_p in best_frames_to_save:\n",
        "                            out_filename = f\"{vid_path.stem}__{video_label}__frame{frame_p.stem.split('frame')[-1]}_sharp{sharpness:.0f}.jpg\"\n",
        "                            out_path = video_best_frames_dir / out_filename\n",
        "                            shutil.copy(str(frame_p), str(out_path))\n",
        "                            update_image_exif(out_path, video_label)\n",
        "                            all_best_frames_paths.append(str(out_path.resolve()))\n",
        "\n",
        "                # 6. Scrive i risultati aggregati del video (con tutte le classi) nel file CSV.\n",
        "                writer.writerow({\n",
        "                    \"SourceType\": \"Video\",\n",
        "                    \"PredictedPlace\": \",\".join(final_video_labels),\n",
        "                    \"Confidence\": \",\".join(confidence_scores),\n",
        "                    \"OriginalPath\": str(vid_path.resolve()),\n",
        "                    \"OutputPath\": \";\".join(all_best_frames_paths),\n",
        "                    \"IsVideo\": 1,\n",
        "                    \"Sharpness\": \",\".join(sharpness_scores)\n",
        "                })\n",
        "\n",
        "                mark_processed(key)\n",
        "                shutil.rmtree(temp_video_frame_dir, ignore_errors=True)\n",
        "        else:\n",
        "            logging.warning(f\"La directory dei video {vid_dir} non esiste.\")\n",
        "\n",
        "    logging.info(\"Processo di Inferenza Concluso\")"
      ],
      "metadata": {
        "id": "JW_dXv8stkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Esecuzione dell'Inferenza**\n",
        "\n",
        "Questa cella avvia il processo di inferenza. Il suo scopo è configurare tutti i parametri necessari, avviare l'analisi e gestire eventuali errori in modo controllato, fornendo un feedback chiaro all'utente.\n",
        "\n",
        "#### **1. Pulizia dello Stato di Avanzamento (Reset)**\n",
        "\n",
        "```python\n",
        "if log_file_path.exists():\n",
        "    log_file_path.unlink()\n",
        "```\n",
        "All'inizio della cella, viene deliberatamente cancellato il file `processed.log`, se esiste.\n",
        "*  Garantisce che, ogni volta che si esegue questa cella, lo script analizzerà di nuovo **tutti** i file presenti nelle directory di input, anche se erano già stati processati in una sessione precedente. Questo è utile per testare modifiche o per assicurarsi di avere un set di risultati completo e aggiornato.\n",
        "*   **Alternativa**: Se si volesse analizzare solo i file nuovi aggiunti tra un'esecuzione e l'altra, basterebbe commentare o rimuovere questo blocco di pulizia. Il sistema diventerebbe così **incrementale**, risparmiando tempo su grandi dataset.\n",
        "\n",
        "#### **2. Configurazione dei Parametri (`args_infer_config`)**\n",
        "\n",
        "Raccoglie tutti i parametri di configurazione in un unico oggetto per passarlo in modo pulito e ordinato alla funzione `infer`.\n",
        "* Viene utilizzato `SimpleNamespace`, per creare un oggetto \"contenitore\". Invece di passare dieci argomenti separati alla funzione `infer`, le passiamo solo `args_infer_config`. All'interno della funzione, si accederà ai valori con una notazione chiara come `args.img_dir` o `args.sharpness_threshold`.\n",
        "*   **Parametri Inclusi**:\n",
        "    *   **Percorsi di Input/Output**: Dove trovare le immagini/video e dove salvare i risultati.\n",
        "    *   **Iperparametri di Inferenza**: Parametri specifici per l'analisi, come l'intervallo di estrazione dei frame (`FRAME_INTERVAL`) o la soglia di nitidezza (`SHARPNESS_THRESHOLD`)."
      ],
      "metadata": {
        "id": "_BieqFL4UIqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Blocco di Pulizia del Log di Processamento\n",
        "# A differenza del blocco di pulizia nella cella del training, questo è attivo di default.\n",
        "# Il suo scopo è cancellare il file `processed.log` prima di ogni esecuzione.\n",
        "# Questo assicura che l'inferenza venga eseguita su TUTTI i file presenti nelle\n",
        "# cartelle di input, anche se sono già stati analizzati in una sessione precedente.\n",
        "# Se volessi un comportamento \"incrementale\" (analizzare solo i file nuovi),\n",
        "# dovresti commentare o rimuovere questo blocco.\n",
        "\n",
        "# Definisce il percorso del file di log.\n",
        "log_file_path = DEFAULT_OUTPUT / \"processed.log\"\n",
        "# Controlla se il file esiste.\n",
        "if log_file_path.exists():\n",
        "    # Stampa messaggi informativi per l'utente.\n",
        "    print(f\"Cancellazione del vecchio file di log: {log_file_path}\")\n",
        "    # Cancella il file.\n",
        "    log_file_path.unlink()\n",
        "    print(\"Log cancellato. Tutti i file verranno ri-processati.\")\n",
        "\n",
        "\n",
        "# Configurazione per l'Inferenza\n",
        "# Crea un oggetto `SimpleNamespace` per contenere tutti i parametri da passare\n",
        "# alla funzione `infer`. Questo raggruppa ordinatamente la configurazione.\n",
        "args_infer_config = SimpleNamespace(\n",
        "    # Percorsi delle directory di input per immagini e video.\n",
        "    img_dir=str(DEFAULT_IMG_DIR),\n",
        "    vid_dir=str(DEFAULT_VID_DIR),\n",
        "\n",
        "    # Percorsi di output.\n",
        "    output=DEFAULT_OUTPUT,\n",
        "    csv_path=DEFAULT_CSV,\n",
        "\n",
        "    # Parametri specifici per il processamento dei video.\n",
        "    frame_interval=FRAME_INTERVAL,\n",
        "    sharpness_threshold=SHARPNESS_THRESHOLD,\n",
        "    frames_per_place_per_video=FRAMES_PER_PLACE_PER_VIDEO,\n",
        ")\n",
        "\n",
        "# Stampa di Verifica dei Parametri\n",
        "# Stampa un riepilogo delle configurazioni chiave per l'inferenza.\n",
        "# È una buona pratica per avere una conferma visiva che lo script\n",
        "# sta leggendo e scriverà nelle cartelle corrette.\n",
        "print(\"\\n--- Parametri per l'Inferenza ---\")\n",
        "print(f\"Directory Immagini Input: {args_infer_config.img_dir}\")\n",
        "print(f\"Directory Video Input: {args_infer_config.vid_dir}\")\n",
        "print(f\"Directory Output: {args_infer_config.output}\")\n",
        "print(f\"File CSV Risultati: {args_infer_config.csv_path}\")\n",
        "\n",
        "# ESECUZIONE DELL'INFERENZA\n",
        "try:\n",
        "    # Chiama la funzione `infer` con la configurazione appena creata.\n",
        "    # A questo punto, lo script inizierà a caricare il modello e a processare\n",
        "    # i file uno per uno, come definito nella cella 10.\n",
        "    infer(args_infer_config)\n",
        "\n",
        "    # Se la funzione `infer` termina senza errori, stampa un messaggio di successo\n",
        "    # e un riepilogo di dove trovare i risultati. Questo è molto utile per l'utente.\n",
        "    print(f\"\\n--- INFERENZA COMPLETATA ---\")\n",
        "    print(f\"Controlla i risultati nel file CSV: {args_infer_config.csv_path}\")\n",
        "    print(f\"Le immagini classificate si trovano in: {args_infer_config.output / 'classified_images'}\")\n",
        "    print(f\"I frame migliori dei video si trovano in: {BEST_FRAMES_OUTPUT_DIR}\")\n",
        "\n",
        "# Cattura qualsiasi errore imprevisto durante l'inferenza.\n",
        "except Exception as e:\n",
        "    # Stampa un messaggio di errore e la traccia completa dello stack per facilitare il debug.\n",
        "    print(f\"ERRORE INASPETTATO DURANTE L'INFERENZA: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "vozCbw2OtqeY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUOVA CELLA - DEFINIZIONE DELLA FUNZIONE DI TEST\n",
        "\n",
        "def test(args):\n",
        "    \"\"\"Esegue la valutazione del modello su un set di dati di test separato.\"\"\"\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"--- Inizio Processo di Test ---\")\n",
        "\n",
        "    model_path = args.output / \"place_model.keras\"\n",
        "    if not model_path.exists():\n",
        "        logging.error(f\"Modello non trovato in: {model_path}. Esegui prima il training.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model = models.load_model(model_path)\n",
        "        logging.info(f\"Modello caricato da: {model_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore caricamento modello: {e}\")\n",
        "        return\n",
        "\n",
        "    test_dir_path = Path(args.test_dir)\n",
        "    if not test_dir_path.is_dir():\n",
        "        logging.error(f\"Directory di test non trovata: {test_dir_path}\")\n",
        "        return\n",
        "\n",
        "    test_ds, class_names = make_dataset(test_dir_path, shuffle=False, augment=False)\n",
        "    if not test_ds:\n",
        "        logging.error(\"Creazione del dataset di test fallita.\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Dataset di test caricato con classi: {class_names}\")\n",
        "    logging.info(\"Valutazione del modello sul set di test in corso...\")\n",
        "\n",
        "    results = model.evaluate(test_ds, verbose=1)\n",
        "\n",
        "    logging.info(\"--- Risultati del Test ---\")\n",
        "    print(\"\\n--- Risultati del Test ---\")\n",
        "    print(f\"Loss sul set di test: {results[0]:.4f}\")\n",
        "    print(f\"Accuracy sul set di test: {results[1] * 100:.2f}%\")\n",
        "    logging.info(f\"Loss: {results[0]:.4f} - Accuracy: {results[1] * 100:.2f}%\")\n",
        "    logging.info(\"--- Processo di Test Concluso ---\")\n",
        "\n",
        "print(\"Funzione di Test definita.\")"
      ],
      "metadata": {
        "id": "9jPYFrCTjZ_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUOVA CELLA - ESECUZIONE DEL TEST - DA CONTROLLARE\n",
        "\n",
        "args_test_config = SimpleNamespace(\n",
        "    test_dir=str(DEFAULT_TEST_DIR),\n",
        "    output=DEFAULT_OUTPUT\n",
        ")\n",
        "\n",
        "try:\n",
        "    print(\"\\nLancio della valutazione sul set di test...\")\n",
        "    test(args_test_config)\n",
        "except Exception as e:\n",
        "    print(f\"ERRORE INASPETTATO DURANTE IL TEST: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "4AWJvNctjblE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}