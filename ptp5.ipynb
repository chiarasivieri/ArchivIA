{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SrI-bwVzshRa"
      },
      "outputs": [],
      "source": [
        "!pip install piexif -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import piexif\n",
        "import shutil\n",
        "from types import SimpleNamespace"
      ],
      "metadata": {
        "id": "b1oKcFoks83R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "7BO8FPOGtHoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# --- Configurazione Percorso Base del Progetto ---\n",
        "# Questo deve puntare alla cartella principale del tuo progetto.\n",
        "GDRIVE_PROJECT_PATH = Path(\"/content/drive/MyDrive/ProgettoClassificazioneLuoghi\")\n",
        "print(f\"Percorso base del progetto impostato a: {GDRIVE_PROJECT_PATH}\")\n",
        "\n",
        "# --- CONFIGURAZIONE PARAMETRI GLOBALI ---\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_HEAD = 10       # Epoche per addestrare solo la testa\n",
        "EPOCHS_FINE = 8        # Epoche per il fine-tuning di tutto il modello\n",
        "FRAME_INTERVAL = 1\n",
        "LEARN_HEAD_LR = 1e-3\n",
        "LEARN_FULL_LR = 1e-5   # Un learning rate molto basso è più sicuro per il fine-tuning\n",
        "\n",
        "# Parametri per selezione frame video\n",
        "SHARPNESS_THRESHOLD = 80.0\n",
        "FRAMES_PER_PLACE_PER_VIDEO = 5\n",
        "\n",
        "# --- PERCORSI DI DEFAULT DERIVATI ---\n",
        "DEFAULT_TRAIN_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"train\"\n",
        "DEFAULT_VAL_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"val\"\n",
        "DEFAULT_IMG_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"inputIMG\"\n",
        "DEFAULT_VID_DIR = GDRIVE_PROJECT_PATH / \"dataset\" / \"inputVID\"\n",
        "\n",
        "DEFAULT_OUTPUT = GDRIVE_PROJECT_PATH / \"output\"\n",
        "DEFAULT_CSV = DEFAULT_OUTPUT / \"results.csv\"\n",
        "PROCESSED_LOG = DEFAULT_OUTPUT / \"processed.log\"\n",
        "BEST_FRAMES_OUTPUT_DIR = DEFAULT_OUTPUT / \"best_frames\"\n",
        "TEMP_FRAME_EXTRACT_DIR = DEFAULT_OUTPUT / \"temp_frames\"\n",
        "\n",
        "# Crea directory di output necessarie\n",
        "DEFAULT_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "BEST_FRAMES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEMP_FRAME_EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Stampa di verifica\n",
        "print(f\"\\n--- Percorsi Configurati ---\")\n",
        "print(f\"Training Data: {DEFAULT_TRAIN_DIR}\")\n",
        "print(f\"Validation Data: {DEFAULT_VAL_DIR}\")\n",
        "print(f\"Input Immagini (Inferenza): {DEFAULT_IMG_DIR}\")\n",
        "print(f\"Input Video (Inferenza): {DEFAULT_VID_DIR}\")\n",
        "print(f\"Output Generale: {DEFAULT_OUTPUT}\")\n",
        "print(f\"-----------------------------\")"
      ],
      "metadata": {
        "id": "sIV5pb3HtJCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging(output_dir: Path):\n",
        "    log_file = output_dir / \"run.log\"\n",
        "    for handler in logging.root.handlers[:]:\n",
        "        logging.root.removeHandler(handler)\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',\n",
        "        handlers=[logging.FileHandler(log_file, encoding='utf-8'), logging.StreamHandler()]\n",
        "    )\n",
        "    logging.info(f\"Logging configurato. Log salvati in: {log_file}\")\n",
        "\n",
        "def is_already_processed(key: str) -> bool:\n",
        "    try:\n",
        "        if not PROCESSED_LOG.exists(): return False\n",
        "        with open(PROCESSED_LOG, 'r', encoding='utf-8') as f:\n",
        "           return any(key == line.strip() for line in f)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore leggendo {PROCESSED_LOG}: {e}\")\n",
        "        return False\n",
        "\n",
        "def mark_processed(key: str):\n",
        "    try:\n",
        "        with open(PROCESSED_LOG, \"a\", encoding='utf-8') as f:\n",
        "            f.write(key + \"\\\\n\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore scrivendo su {PROCESSED_LOG}: {e}\")\n",
        "\n",
        "def extract_frames(video_path: Path, frame_output_base_dir: Path, interval_sec: int) -> list:\n",
        "    saved_frames_paths = []\n",
        "    video_frame_dir = frame_output_base_dir / video_path.stem\n",
        "    video_frame_dir.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        if not cap.isOpened():\n",
        "            logging.error(f\"Impossibile aprire il video: {video_path}\")\n",
        "            return []\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "        step = max(1, int(fps * interval_sec))\n",
        "        frame_count, saved_frame_index, total_frames = 0, 0, int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        with tqdm(total=total_frames, desc=f\"Extracting frames from {video_path.name}\") as pbar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret: break\n",
        "                if frame_count % step == 0:\n",
        "                    out_file_path = video_frame_dir / f\"{video_path.stem}_frame{saved_frame_index:05d}.jpg\"\n",
        "                    if cv2.imwrite(str(out_file_path), frame, [cv2.IMWRITE_JPEG_QUALITY, 90]):\n",
        "                        saved_frames_paths.append(out_file_path)\n",
        "                        saved_frame_index += 1\n",
        "                frame_count += 1\n",
        "                pbar.update(1)\n",
        "        cap.release()\n",
        "        logging.info(f\"Estratti {len(saved_frames_paths)} frame da {video_path.name}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Errore durante estrazione frame da {video_path}: {e}\")\n",
        "    return saved_frames_paths\n",
        "\n",
        "def update_image_exif(img_path: Path, label: str):\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        exif_data = img.info.get('exif', b\"\")\n",
        "        exif_dict = piexif.load(exif_data) if exif_data else {'0th': {}}\n",
        "        if '0th' not in exif_dict: exif_dict['0th'] = {}\n",
        "        exif_dict['0th'][piexif.ImageIFD.ImageDescription] = label.encode('utf-8')\n",
        "        exif_dict['thumbnail'] = None\n",
        "        exif_bytes = piexif.dump(exif_dict)\n",
        "        img.save(img_path, exif=exif_bytes, quality=95)\n",
        "        img.close()\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"UPDATE_EXIF: Errore per {img_path}: {e}\")\n",
        "\n",
        "def calculate_sharpness(image_cv2):\n",
        "    try:\n",
        "        if image_cv2 is None or image_cv2.size == 0: return 0.0\n",
        "        gray = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2GRAY)\n",
        "        return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"CALCULATE_SHARPNESS: Errore: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "print(\"Funzioni di Utilità definite.\")"
      ],
      "metadata": {
        "id": "KtwQka6qtOST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funzione per costruire il modello EfficientNetV2 ---\n",
        "def build_model(num_classes: int) -> models.Model:\n",
        "    \"\"\"Costruisce un modello di classificazione basato su EfficientNetV2B0 pre-addestrato.\"\"\"\n",
        "    # Se vuoi provare un modello più grande, cambia EfficientNetV2B0 qui.\n",
        "    # Esempi: EfficientNetV2B1, EfficientNetV2B2\n",
        "    backbone = EfficientNetV2B0(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_shape=(*IMG_SIZE, 3),\n",
        "        drop_connect_rate=0.2  # Aggiunge regolarizzazione\n",
        "    )\n",
        "\n",
        "    backbone.trainable = False # Congela il backbone per il training della testa\n",
        "\n",
        "    # Costruisci la testa del modello\n",
        "    x = layers.GlobalAveragePooling2D(name=\"gap\")(backbone.output)\n",
        "    x = layers.Dropout(0.3, name=\"dropout\")(x) # Dropout per regolarizzazione\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"predictions\")(x)\n",
        "    model = models.Model(backbone.input, outputs, name=\"efficientnetv2b0_finetuned\")\n",
        "\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=LEARN_HEAD_LR),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    logging.info(f\"Modello EfficientNetV2B0 costruito e compilato per il training della testa.\")\n",
        "    return model\n",
        "\n",
        "# --- Pipeline di Data Augmentation ---\n",
        "data_augmentation_pipeline = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.15),\n",
        "    layers.RandomZoom(0.15),\n",
        "    layers.RandomContrast(0.1),\n",
        "    layers.RandomBrightness(0.1),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# --- Funzione per creare i dataset ---\n",
        "def make_dataset(dirpath: Path, shuffle: bool, subset: str = None, validation_split: float = None, augment: bool = False):\n",
        "    if not dirpath.is_dir():\n",
        "        logging.error(f\"MAKE_DATASET: Directory non trovata: {dirpath}\")\n",
        "        return None, None\n",
        "    try:\n",
        "        initial_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            dirpath, labels=\"inferred\", label_mode=\"categorical\",\n",
        "            batch_size=BATCH_SIZE, image_size=IMG_SIZE, shuffle=shuffle,\n",
        "            seed=123, validation_split=validation_split, subset=subset,\n",
        "            interpolation='bilinear'\n",
        "        )\n",
        "        class_names = initial_ds.class_names\n",
        "\n",
        "        # Applica augmentation se richiesto (solo per il training set)\n",
        "        processed_ds = initial_ds\n",
        "        if augment:\n",
        "            processed_ds = processed_ds.map(lambda imgs, labs: (data_augmentation_pipeline(imgs, training=True), labs),\n",
        "                                           num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Applica sempre la funzione di preprocessing specifica del modello\n",
        "        processed_ds = processed_ds.map(lambda imgs, labs: (preprocess_input(imgs), labs),\n",
        "                                       num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        return processed_ds.prefetch(tf.data.AUTOTUNE), class_names\n",
        "    except Exception as e:\n",
        "        logging.error(f\"MAKE_DATASET: Errore durante creazione dataset da {dirpath}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "print(\"Funzioni per Modello e Dataset definite.\")"
      ],
      "metadata": {
        "id": "N6wZX4RetTay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    \"\"\"Esegue il ciclo di training completo.\"\"\"\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"--- Inizio Processo di Training ---\")\n",
        "\n",
        "    # Gestione dataset di training e validazione\n",
        "    train_dir_path = Path(args.train_dir)\n",
        "    val_dir_path = Path(args.val_dir) if args.val_dir else None\n",
        "\n",
        "    if val_dir_path and val_dir_path.is_dir():\n",
        "        logging.info(f\"Uso directory di validazione separata: {val_dir_path}\")\n",
        "        train_ds, class_names = make_dataset(train_dir_path, shuffle=True, augment=True)\n",
        "        val_ds, _ = make_dataset(val_dir_path, shuffle=False)\n",
        "    else:\n",
        "        logging.info(f\"Uso 20% split da {train_dir_path} per validazione.\")\n",
        "        train_ds, class_names = make_dataset(train_dir_path, shuffle=True, subset='training', validation_split=0.2, augment=True)\n",
        "        val_ds, _ = make_dataset(train_dir_path, shuffle=False, subset='validation', validation_split=0.2)\n",
        "\n",
        "    if not all([train_ds, val_ds, class_names]):\n",
        "        logging.error(\"Creazione dataset fallita. Training interrotto.\")\n",
        "        return\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    logging.info(f\"Trovate {num_classes} classi: {class_names}\")\n",
        "\n",
        "    # Costruisci il modello\n",
        "    model = build_model(num_classes)\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint_head = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(args.output / \"model_head_best.keras\"), monitor='val_accuracy',\n",
        "        save_best_only=True, mode='max', verbose=1)\n",
        "    checkpoint_fine = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=str(args.output / \"model_fine_tuned_best.keras\"), monitor='val_accuracy',\n",
        "        save_best_only=True, mode='max', verbose=1)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "    # Fase 1: Training della testa\n",
        "    logging.info(f\"--- FASE 1: Training testa per {args.epochs_head} epoche ---\")\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=args.epochs_head, callbacks=[checkpoint_head])\n",
        "\n",
        "    # Carica il miglior modello dalla fase 1\n",
        "    logging.info(\"Caricamento del miglior modello dal training della testa.\")\n",
        "    model = models.load_model(args.output / \"model_head_best.keras\")\n",
        "\n",
        "    # Fase 2: Fine-tuning\n",
        "    model.layers[0].trainable = True # Scongela il backbone EfficientNetV2\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=args.learn_full_lr),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    logging.info(f\"Modello ricompilato per fine-tuning (LR={args.learn_full_lr})\")\n",
        "\n",
        "    logging.info(f\"--- FASE 2: Fine-tuning per max {args.epochs_fine} epoche ---\")\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=args.epochs_fine, callbacks=[checkpoint_fine, early_stopping])\n",
        "\n",
        "    # Salvataggio modello finale e classi\n",
        "    final_model_path = args.output / \"place_model.keras\"\n",
        "    logging.info(f\"Salvataggio modello finale in: {final_model_path}\")\n",
        "    model.save(final_model_path)\n",
        "\n",
        "    class_indices_path = args.output / \"class_indices.csv\"\n",
        "    pd.DataFrame({\"class_name\": class_names, \"index\": list(range(num_classes))}).to_csv(class_indices_path, index=False)\n",
        "    logging.info(f\"Mappatura classi salvata in: {class_indices_path}\")\n",
        "    logging.info(\"--- Processo di Training Concluso ---\")\n",
        "\n",
        "print(\"Funzione di Training definita.\")"
      ],
      "metadata": {
        "id": "Tj4VxSfgtWgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prima di iniziare un nuovo training, cancellare i vecchi modelli\n",
        "# e il log dei file processati per partire da zero.\n",
        "# De-commenta le righe seguenti se vuoi fare pulizia.\n",
        "\n",
        "# print(\"Pulizia dei vecchi file di modello e log in corso...\")\n",
        "# old_model_files = [\"model_head_best.keras\", \"model_fine_tuned_best.keras\", \"place_model.keras\"]\n",
        "# for f_name in old_model_files:\n",
        "#     if (DEFAULT_OUTPUT / f_name).exists():\n",
        "#         (DEFAULT_OUTPUT / f_name).unlink()\n",
        "# if PROCESSED_LOG.exists():\n",
        "#     PROCESSED_LOG.unlink()\n",
        "# print(\"Pulizia completata.\")\n",
        "\n",
        "\n",
        "# Configurazione per il training\n",
        "args_train_config = SimpleNamespace(\n",
        "    train_dir=str(DEFAULT_TRAIN_DIR),\n",
        "    val_dir=str(DEFAULT_VAL_DIR),\n",
        "    output=DEFAULT_OUTPUT,\n",
        "    epochs_head=EPOCHS_HEAD,\n",
        "    epochs_fine=EPOCHS_FINE,\n",
        "    learn_head_lr=LEARN_HEAD_LR,\n",
        "    learn_full_lr=LEARN_FULL_LR\n",
        ")\n",
        "\n",
        "# ESECUZIONE DEL TRAINING\n",
        "try:\n",
        "    train(args_train_config)\n",
        "except Exception as e:\n",
        "    print(f\"ERRORE INASPETTATO DURANTE IL TRAINING: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "fufQWEKita2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(args):\n",
        "    \"\"\"Esegue inferenza su immagini e video.\"\"\"\n",
        "    setup_logging(args.output)\n",
        "    logging.info(\"--- Inizio Processo di Inferenza ---\")\n",
        "\n",
        "    # Caricamento modello e classi\n",
        "    model_path = args.output / \"place_model.keras\"\n",
        "    class_indices_path = args.output / \"class_indices.csv\"\n",
        "    if not model_path.exists() or not class_indices_path.exists():\n",
        "        logging.error(f\"Modello ({model_path}) o indici classi ({class_indices_path}) non trovati.\")\n",
        "        return\n",
        "\n",
        "    model = models.load_model(model_path)\n",
        "    class_names = pd.read_csv(class_indices_path).sort_values(\"index\")[\"class_name\"].tolist()\n",
        "    logging.info(f\"Modello e {len(class_names)} classi caricate.\")\n",
        "\n",
        "    # Preparazione file CSV\n",
        "    with open(args.csv_path, \"a\", newline=\"\", encoding='utf-8') as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=[\"SourceType\", \"PredictedPlace\", \"OriginalPath\", \"Confidence\", \"IsVideo\", \"BestFrameSavedPath\", \"Sharpness\"])\n",
        "        if csv_file.tell() == 0:\n",
        "            writer.writeheader()\n",
        "\n",
        "        # Funzione helper per caricare e preprocessare una singola immagine\n",
        "        def load_and_preprocess_img_infer(path_str):\n",
        "            try:\n",
        "                img = tf.io.read_file(path_str)\n",
        "                img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "                img = tf.image.resize(img, IMG_SIZE)\n",
        "                img = preprocess_input(img)\n",
        "                return img\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Errore caricamento immagine {path_str}: {e}\")\n",
        "                return None\n",
        "\n",
        "        # Processamento VIDEO\n",
        "        logging.info(f\"Inferenza video in: {args.vid_dir}\")\n",
        "        vid_dir = Path(args.vid_dir)\n",
        "        if vid_dir.is_dir():\n",
        "            video_files = sorted(list(vid_dir.glob(\"*.mp4\")) + list(vid_dir.glob(\"*.mov\")) + list(vid_dir.glob(\"*.avi\")))\n",
        "            for vid_path in video_files:\n",
        "                key = f\"VID::{vid_path.resolve()}\"\n",
        "                if is_already_processed(key):\n",
        "                    logging.info(f\"Video già processato, saltato: {vid_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                logging.info(f\"Processando video: {vid_path.name}\")\n",
        "                frame_paths = extract_frames(vid_path, TEMP_FRAME_EXTRACT_DIR, args.frame_interval)\n",
        "                if not frame_paths: continue\n",
        "\n",
        "                # Carica e preprocessa i frame validi\n",
        "                valid_frames_data = [d for d in [load_and_preprocess_img_infer(str(p)) for p in frame_paths] if d is not None]\n",
        "                if not valid_frames_data: continue\n",
        "\n",
        "                # Esegui predizioni\n",
        "                predictions = model.predict(tf.stack(valid_frames_data), batch_size=BATCH_SIZE)\n",
        "                pred_indices = np.argmax(predictions, axis=1)\n",
        "                pred_confidences = np.max(predictions, axis=1)\n",
        "\n",
        "                # Calcola risultato per il video\n",
        "                majority_idx = np.bincount(pred_indices).argmax()\n",
        "                video_label = class_names[majority_idx]\n",
        "                video_confidence = float(np.mean(pred_confidences[pred_indices == majority_idx]))\n",
        "\n",
        "                logging.info(f\"Video: {vid_path.name} -> Classe: {video_label} (Conf: {video_confidence:.3f})\")\n",
        "\n",
        "                # Seleziona e salva i frame migliori\n",
        "                candidate_frames_info = []\n",
        "                for i, frame_path in enumerate(frame_paths):\n",
        "                    if pred_indices[i] == majority_idx:\n",
        "                        sharpness = calculate_sharpness(cv2.imread(str(frame_path)))\n",
        "                        if sharpness >= args.sharpness_threshold:\n",
        "                            candidate_frames_info.append((sharpness, frame_path, pred_confidences[i]))\n",
        "\n",
        "                candidate_frames_info.sort(key=lambda x: x[0], reverse=True)\n",
        "                best_frames_to_save = candidate_frames_info[:args.frames_per_place]\n",
        "\n",
        "                saved_best_frame_paths = []\n",
        "                if best_frames_to_save:\n",
        "                    video_best_frames_dir = BEST_FRAMES_OUTPUT_DIR / video_label / vid_path.stem\n",
        "                    video_best_frames_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    for sharpness, frame_p, conf in best_frames_to_save:\n",
        "                        out_filename = f\"{vid_path.stem}__{video_label}__frame{frame_p.stem.split('frame')[-1]}_sharp{sharpness:.0f}.jpg\"\n",
        "                        out_path = video_best_frames_dir / out_filename\n",
        "                        shutil.copy(str(frame_p), str(out_path))\n",
        "                        update_image_exif(out_path, video_label)\n",
        "                        saved_best_frame_paths.append(str(out_path.resolve()))\n",
        "\n",
        "                writer.writerow({\n",
        "                    \"SourceType\": \"Video\", \"PredictedPlace\": video_label, \"OriginalPath\": str(vid_path.resolve()),\n",
        "                    \"Confidence\": f\"{video_confidence:.4f}\", \"IsVideo\": 1,\n",
        "                    \"BestFrameSavedPath\": \";\".join(saved_best_frame_paths) if saved_best_frame_paths else \"N/A\",\n",
        "                    \"Sharpness\": f\"{best_frames_to_save[0][0]:.1f}\" if best_frames_to_save else \"N/A\"\n",
        "                })\n",
        "                mark_processed(key)\n",
        "                shutil.rmtree(TEMP_FRAME_EXTRACT_DIR / vid_path.stem, ignore_errors=True)\n",
        "\n",
        "    logging.info(\"--- Processo di Inferenza Concluso ---\")\n",
        "\n",
        "print(\"Funzione di Inferenza definita.\")"
      ],
      "metadata": {
        "id": "JW_dXv8stkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args_infer_config = SimpleNamespace(\n",
        "    img_dir=str(DEFAULT_IMG_DIR),\n",
        "    vid_dir=str(DEFAULT_VID_DIR),\n",
        "    output=DEFAULT_OUTPUT,\n",
        "    csv_path=DEFAULT_CSV,\n",
        "    frame_interval=FRAME_INTERVAL,\n",
        "    sharpness_threshold=SHARPNESS_THRESHOLD,\n",
        "    frames_per_place=FRAMES_PER_PLACE_PER_VIDEO,\n",
        ")\n",
        "\n",
        "print(\"--- Parametri per l'Inferenza ---\")\n",
        "print(f\"Directory Video Input: {args_infer_config.vid_dir}\")\n",
        "print(f\"Directory Output: {args_infer_config.output}\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# ESECUZIONE DELL'INFERENZA\n",
        "try:\n",
        "    infer(args_infer_config)\n",
        "except Exception as e:\n",
        "    print(f\"ERRORE INASPETTATO DURANTE L'INFERENZA: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "vozCbw2OtqeY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}